<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Thanh Nguyen-Tang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<div id="layout-content">
</br>
 <h1>Thanh Nguyen-Tang</h1>
 <img alt="tnt" src="profile_picture.png"
style="display:inline;margin:1px 15px 1px 1px;float:center" width="140"  id="profileimage">
<div id="subtitle">Postdoctoral Research Fellow <br /> 
<a href="https://www.cs.jhu.edu/" target=&ldquo;blank&rdquo;>Department of Computer Science</a> <br /> 
<a href="https://engineering.jhu.edu/" target=&ldquo;blank&rdquo;>Whiting School of Engineering</a> <br /> 
<a href="https://www.jhu.edu/" target=&ldquo;blank&rdquo;>Johns Hopkins University</a> <br /> 
Malone Hall 345, 3400 N Charles Street, Baltimore, MD 21218 <br /> 
<i>nguyent</i> at cs dot jhu dot edu / <i>thnguyentang</i> at gmail dot com <br /> 
[<a href="https://scholar.google.com/citations?hl=en&amp;user=UrTlMiwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=&ldquo;blank&rdquo;>Google Scholar</a>] [<a href="https://github.com/thanhnguyentang" target=&ldquo;blank&rdquo;>Github</a>]  [<a href="https://almostcompletenotes.wordpress.com/" target=&ldquo;blank&rdquo;>blog</a>] <br /><br />
</div>
<p><font color=red > *I'm on the 2024-2025 job market [<a href="preprints/research_statement.pdf">research statement</a>].</font>
</p>
<h2>Background</h2>
<p>I am currently a postdoc at Johns Hopkins University (with <a href="https://www.cs.jhu.edu/~raman/Home.html" target=&ldquo;blank&rdquo;>Raman Arora</a>). 
Prior to that, I did my PhD in Computer Science at <a href="https://a2i2.deakin.edu.au/publications/" target=&ldquo;blank&rdquo;>The Applied AI Institute</a>, Deakin University, Australia 
(Alfred Deakin Medal for Doctoral Theses). 
I did my M.Sc. in Computer Science at Ulsan National Institute of Science and Technology, South Korea. 
In my previous life, I studied Electronic and Communication Engineering (Talented Engineering Program) at Danang University of Science and Technology, Vietnam. 
</p>
<h2>Research interest </h2>
<p><font color=red > &#8201;&mdash;&#8201;<i><b>Make the world an \(\epsilon\)-better place</b></i> </font>
</p>
<p>My research is on the theoretical and algorithmic foundations of machine learning for modern data science and AI, with the current focus on the following topics: 
</p>
<ul>
<li><p><b>Transfer decision-making</b> (e.g., offline learning, multi-task/representation learning, federated learning, domain adaptation)
</p>
</li>
<li><p><b>Multi-agent learning</b> (e.g., policy regret minimization, equilibrium computation, mechanism design for learning agents)
</p>
</li>
<li><p><b>Trustworthy AI</b> (e.g., distributional/adversarial robustness, distributional learning, differential privacy)
</p>
</li>
<li><p><b>Large language models</b> (e.g., understanding inductive biases of transformers for emerging abilities such as
in-context learning and reasoning)
</p>
</li>
</ul>
<p><b>Keywords</b>: <i>learning</i>, <i>representation</i>, <i>optimization</i>, <i>computation</i>. 
</p>
<p><b><u>Note</u></b>:
</p>
<ul>
<li><p>Highly motivated and self-driven students with a strong mathematical background are welcome to contact me for research. 
</p>
</li>
<li><p>I welcome and appreciate <a href="https://forms.gle/LBPu7gNDi66VHnxV7" target=&ldquo;blank&rdquo;>anonymous feedback</a> from anyone on anything. 
</p>
</li>
</ul>
<h2>Publications </h2>
<h3>2025 </h3>
<p>23. Anh Tong, <u>Thanh Nguyen-Tang</u>, Dongeun Lee, Duc Nguyen, Toan Tran, David Leo Wright Hall, Cheongwoong Kang, Jaesik Choi. 
<b>Neural ODE transformers: Analyzing internal dynamics and adaptive fine-tuning</b>. <font color=red >ICLR</font>, 2025. <br />
22. Nguyen Hung-Quang, Ngoc-Hieu Nguyen, The-Anh Ta, <u>Thanh Nguyen-Tang</u>, Kok-Seng Wong, Hoang Thanh-Tung,
and Khoa D Doan. <b>Wicked oddities: Selectively poisoning for effective clean-label backdoor attacks</b>. <font color=red >ICLR</font>, 2025 [<a href="https://arxiv.org/pdf/2407.10825" target=&ldquo;blank&rdquo;>pdf</a>]. <br />  
21. Ragja Palakkadavath, Hung Le, <u>Thanh Nguyen-Tang</u>, Svetha Venkatesh, Sunil Gupta. 
<b>Fair domain generalization with heterogeneous sensitive attributes across domains</b>. <font color=red >WACV</font>, 2025 [<a href="https://openreview.net/pdf?id=3wL1tj3kqE" target=&ldquo;blank&rdquo;>pdf</a>].<br /> 
</p>
<h3>2024 </h3>
<p>20. <u>Thanh Nguyen-Tang</u>, Raman Arora. <b>Learning in Markov games with adaptive adversaries: Policy regret, fundamental barriers, and efficient algorithms</b>. <font color=red >NeurIPS</font>, 2024 [<a href="https://arxiv.org/pdf/2411.00707" target=&ldquo;blank&rdquo;>pdf</a>]. <br />
19. Austin Watkins, <u>Thanh Nguyen-Tang</u>, Enayat Ullah, Raman Arora. <b>Adversarially robust multi-task representation learning</b>. <font color=red >NeurIPS</font>, 2024 [<a href="https://openreview.net/pdf?id=w2L3Ll1jbV" target=&ldquo;blank&rdquo;>pdf</a>]. <br />
18. Haque Ishfaq, <u>Thanh Nguyen-Tang</u>, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup. <b>Offline multitask representation learning for reinforcement learning</b>. <font color=red >NeurIPS</font>, 2024 [<a href="https://arxiv.org/pdf/2403.11574" target=&ldquo;blank&rdquo;>pdf</a>].<br />
17. <u>Thanh Nguyen-Tang</u>, Raman Arora. <b>On the statistical complexity of offline decision-making</b>. <font color=red >ICML</font>, 2024 [<a href="https://openreview.net/pdf?id=dYDPcx78tm" target=&ldquo;blank&rdquo;>pdf</a>]. 
</p>
<h3>2023 </h3>
<p>16. Anh Tong, <u>Thanh Nguyen-Tang</u>, Dongeun Lee, Toan Tran, Jaesik Choi. <b>SigFormer: Signature transformers for deep hedging</b>. <font color=red >ICAIF</font>, 2023 (<font color=red >Oral</font>)[<a href="https://arxiv.org/abs/2310.13369" target=&ldquo;blank&rdquo;>pdf]</a>. <br /> 
15. Anh Do, <u>Thanh Nguyen-Tang</u>, Raman Arora. <b>Multi-agent learning with heterogeneous linear contextual bandits</b>. <font color=red >NeurIPS</font>, 2023 [<a href="[https://openreview.net/forum?id=7f6vH3mmhr" target=&ldquo;blank&rdquo;>pdf</a>]. <br />
14. Austin Watkins, Enayat Ullah, <u>Thanh Nguyen-Tang</u>, Raman Arora. <b>Optimistic rates for multi-task representation learning</b>. <font color=red >NeurIPS</font>, 2023 [<a href="https://openreview.net/forum?id=gQ4h6WvME0" target=&ldquo;blank&rdquo;>pdf</a>]<br />
13. <u>Thanh Nguyen-Tang</u>, Raman Arora. <b>On sample-efficient offline reinforcement learning: Data diversity, posterior sampling and beyond</b>. <font color=red >NeurIPS</font>, 2023 [<a href="https://openreview.net/forum?id=sdlh4gVOj8" target=&ldquo;blank&rdquo;>pdf</a>]. <br />
12.  Ragja Palakkadavath, <u>Thanh Nguyen-Tang</u>, Hung Le, Svetha Venkatesh, Sunil Gupta.  <b>Domain generalization with interpolation robustness</b>. <font color=red >ACML</font>, 2023 [<a href="https://openreview.net/pdf?id=Yl_4LpR_3Z" target=&ldquo;blank&rdquo;>pdf</a>]. <br />
11. Thong Bach, Anh Tong, Truong Son Hy, Vu Nguyen, <u>Thanh Nguyen-Tang</u>. <b>Global contrastive learning for long-tailed classification</b>. <font color=red >TMLR</font>, 2023 [<a href="https://openreview.net/forum?id=xWrtiJwJj5" target=&ldquo;blank&rdquo;>pdf</a>]. <br />
10. A. Tuan Nguyen, <u>Thanh Nguyen-Tang</u>, Ser-Nam Lim, Philip Torr. <b>TIPI: Test time adaptation with transformation invariance</b>. <font color=red >CVPR</font>, 2023 [<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2023_paper.html" target=&ldquo;blank&rdquo;>html</a>].  <br />
9. <u>Thanh Nguyen-Tang</u>, Raman Arora. <b>VIPeR: Provably efficient algorithm for offline RL with neural function approximation</b>. <font color=red >ICLR</font>, 2023 (<font color=red >top 25% noble</font>).
[<a href="https://recorder-v3.slideslive.com/?share=80688&amp;s=a24d555f-edda-4210-a19e-4fded4165c62" target=&ldquo;blank&rdquo;>talk</a>] [<a href="assets/viper.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="https://github.com/thanhnguyentang/neural-offline-rl" target=&ldquo;blank&rdquo;>code</a>]  [<a href="notes/erratumiclr23.html" target=&ldquo;blank&rdquo;><b>ERRATUM</b></a>.] <br />
8.  <u>Thanh Nguyen-Tang</u>, Ming Yin, Sunil Gupta, Svetha Venkatesh, Raman Arora. <b>On instance-dependent bounds for offline reinforcement learning with linear function approximation</b>. <font color=red >AAAI</font>, 2023 [<a href="https://arxiv.org/abs/2211.13208" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="assets/aaai23-poster.pdf" target=&ldquo;blank&rdquo;>poster</a>] [<a href="assets/aaai23-slides-only.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="https://youtu.be/xUsMHodydO8" target=&ldquo;blank&rdquo;>video</a>].
</p>
<h3>2022   </h3>
<p>7. Anh Tong, <u>Thanh Nguyen-Tang</u>, Toan Tran, Jaesik Choi. <b>Learning fractional white noises in neural stochastic differential equations</b>. <font color=red >NeurIPS</font>, 2022 [<a href="https://openreview.net/pdf?id=lTZBRxm2q5" target=&ldquo;blank&rdquo;>pdf</a>] 
[<a href="https://github.com/anh-tong/fractional_neural_sde" target=&ldquo;blank&rdquo;>code</a>].  <br />
6. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, A.Tuan Nguyen, and Svetha Venkatesh. <b>Offline neural contextual bandits:  Pessimism, optimization, and generalization</b>. <font color=red >ICLR</font>, 2022 [<a href="https://openreview.net/pdf?id=sPIFuucA3F" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="assets/poster_NeurIPSW21.pdf" target=&ldquo;blank&rdquo;>poster</a>]  
[<a href="assets/neuralcb_slides.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://github.com/thanhnguyentang/offline_neural_bandits" target=&ldquo;blank&rdquo;>code</a>]. <br />
5. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Hung Tran-The, Svetha Venkatesh. <b>On sample complexity of offline reinforcement learning with deep ReLU networks in Besov spaces</b>. <font color=red >TMLR</font>, 2022, Workshop on RL Theory, ICML, 2021 [<a href="https://arxiv.org/abs/2103.06671" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://thanhnguyentang.github.io/assets/offrelu.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://www.youtube.com/watch?v=xLM5pondWY4" target=&ldquo;blank&rdquo;>talk</a>]. <br />
</p>
<h3>2021</h3>
<p>4. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Svetha Venkatesh. <b>Distributional reinforcement learning via moment matching</b>. <font color=red >AAAI</font>, 2021 [<a href="http://arxiv.org/abs/2007.12354" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/mmdrl" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://cutt.ly/fkkiAGm" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://cutt.ly/4kkiJZt" target=&ldquo;blank&rdquo;>poster</a>] [<a href="https://youtu.be/1fMqZZjy84E" target=&ldquo;blank&rdquo;>talk</a>].  
</p>
<h3>2020</h3>
<p>3. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Huong Ha, Santu Rana, Svetha Venkatesh. <b>Distributionally robust Bayesian quadrature optimization</b>. <font color=red >AISTATS</font>, 2020 [<a href="https://arxiv.org/abs/2001.06814" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/drbqo" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://thanhnguyentang.github.io/assets/aistats20_drbqo.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://slideslive.com/38930124/" target=&ldquo;blank&rdquo;>talk</a>].  
</p>
<h3>2019</h3>
<p>2. Huong Ha, Santu Rana, Sunil Gupta, <u>Thanh Nguyen-Tang</u>, Hung Tran-The, Svetha Venkatesh. <b>Bayesian optimization with unknown search space</b>. <font color=red >NeurIPS</font>, 2019 [<a href="https://papers.nips.cc/paper/9350-bayesian-optimization-with-unknown-search-space" target=&ldquo;blank&rdquo;>pdf</a>] [<a href="https://github.com/HuongHa12/BO_unknown_searchspace" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://postersession.ai/poster/bayesian-optimization-with-unknown-searc/" target=&ldquo;blank&rdquo;>poster</a>].<br />
1. <u>Thanh Nguyen-Tang</u>, Jaesik Choi. <b>Markov information bottleneck to improve information flow in stochastic neural networks</b>. <font color=red >Entropy</font>, 2019 (Special Issue on Information Bottleneck: Theory and Applications in Deep Learning) [<a href="https://doi.org/10.3390/e21100976" target=&ldquo;blank&rdquo;>pdf</a>].   <br />
</p>
<h2>Mentoring </h2>
<ul>
<li><p><a href="https://www.linkedin.com/in/andrew-gilbert-0466982b5/" target=&ldquo;blank&rdquo;>Andrew Gilbert</a>, sophomore in Computer Science and Applied Mathematics &amp; Statistics at JHU (01/2025-present). <u>Topic</u>: <b>Reinforcement learning</b>  
</p>
</li>
<li><p><a href="https://www.linkedin.com/in/khaileduc/" target=&ldquo;blank&rdquo;>Le Duc Khai</a>, Masters student in Biomedical Engineering at University of Toronto (12/2024-present). <u>Topic</u>: <b>Medical AI and multimodal LLMs</b>
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=pSeyGjMAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Austin Watkins</a>, PhD student at JHU (2022-present). <u>Topic</u>:  <b>Transfer learning and robustness</b>
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=yFLbTtkAAAAJ&amp;hl=en&amp;authuser=1" target=&ldquo;blank&rdquo;>Thong Bach</a>, independent researcher -&gt; PhD student at Deakin (2022-now). <u>Topic</u>: <b>Self-supervised learning in LLMs</b>
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=aB4jrTIAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Anh Do</a>, PhD student at JHU (2022-2024). <u>Topic</u>: <b>Bandit/Reinforcement learning</b>
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=cVTpiuoAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Ragja Palakkadavath</a>, PhD student at Deakin University (2022-2024). <u>Topic</u>: <b>Out-of-distribution generalization</b>   
</p>
</li>
</ul>
<h2>Teaching  </h2>
<ul>
<li><p>Guest lecturer, Machine Learning, JHU CS Spring 2025. 
</p>
</li>
<li><p>Guest lecturer, Learning Theory (EN.601.474.01 : ML) – 36 students, JHU CS Fall 2024
</p>
</li>
<li><p>Co-lecturer, Machine Learning: Advanced Topics (EN.601.779.01.SP24) – 17 graduate students, JHU CS Spring 2024
</p>
</li>
<li><p>Guest lecturer, Machine Learning (EN.601.675.01.SP23) – 77 undergraduate students, JHU CS Spring 2023
</p>
</li>
<li><p>Teaching Assistant, Advanced Machine Learning (CSE 54401), UNIST CSE Fall 2016
</p>
</li>
<li><p>Teaching Assistant, Engineering Programming (ITP117), UNIST CSE Spring 2016
</p>
</li>
<li><p>Teaching Assistant, Linear Algebra, Calculus, Digital Signal Processing, Machine Learning, ECE, DUT, 2011 - 2015
</p>
</li>
</ul>
<p>* I participated in (and obtained a certificate of) Justice, Equity, Diversity, and Inclusion (JEDI) Training in the Classroom in March 2024 at JHU, as an effort to improve diversity in my future classes and research group. 
</p>
<h2>Selected award/honor   </h2>
<ul>
<li><p>Alfred Deakin Medal for Doctoral Theses (for the most outstanding theses), 2022.
</p>
</li>
</ul>
<h2>Independent recognition </h2>
<ul>
<li><p>I am acknowledged in <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target=&ldquo;blank&rdquo;>Francis Bach's book, &ldquo;Learning Theory from First Principles&rdquo;</a>
</p>
</li>
<li><p>My AAAI&rsquo;21 paper is featured as an excercise in <a href="https://mitpress.mit.edu/9780262048019/distributional-reinforcement-learning/" target=&ldquo;blank&rdquo;>Bellemare, Dabney, and Rowland's book, &ldquo;Distributional Reinforcement Learning&rdquo;</a>
</p>
</li>
</ul>
<h2>Professional service  </h2>
<p>Area Chair/Senior Program Committee
</p>
<ul>
<li><p>International Conference on Artificial Intelligence and Statistics (AISTATS) 2025
</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence (AAAI) 2025, 2024, 2023
</p>
</li>
</ul>
<p>Conference Reviewer/Program Committee
</p>
<ul>
<li><p>Neural Information Processing Systems (NeurIPS) 2024, 2023, 2022, 2021, 2020
</p>
</li>
<li><p>International Conference on Machine Learning (ICML) 2025, 2023, 2022, 2021
</p>
</li>
<li><p>International Conference on Learning Representations (ICLR) 2024, 2023, 2022, 2021 (outstanding reviewer award)
</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence (AAAI) 2022, 2021 (top 25% reviewer)
</p>
</li>
<li><p>International Conference on Artificial Intelligence and Statistics (AISTATS) 2021
</p>
</li>
<li><p>Annual Learning for Dynamics &amp; Control Conference (L4DC) 2022
</p>
</li>
</ul>
<p>Coordinator
</p>
<ul>
<li><p>AAAI Conference on Artificial Intelligence (AAAI) 2023 (session chair for ML theory)
</p>
</li>
<li><p>International Conference on Machine Learning (ICML) 2022
</p>
</li>
<li><p>International Conference on Automated Machine Learning (AutoML) 2022
</p>
</li>
</ul>
<h2>Invited talks </h2>
<ul>
<li><p>TrustML Young Scientist Seminars, RIKEN Japan, Aug. 01, 2023 [<a href="https://trustmlresearch.github.io/seminar-talks/index_Thanh_Nguyen.html" target=&ldquo;blank&rdquo;>post</a>] [<a href="assets/riken23.pdf">slides</a>] [<a href="https://www.youtube.com/watch?v=Z-r2XmxLAgk" target=&ldquo;blank&rdquo;>video</a>].
</p>
</li>
<li><p>VinAI, Vietnam, Jan. 13, 2023 [<a href="https://www.vinai.io/seminar-posts/provable-offline-reinforcement-learning-neural-function-approximation-randomization-and-sample-complexity/" target=&ldquo;blank&rdquo;>post</a>].
</p>
</li>
<li><p>FPT AI, Vietnam, Dec. 21, 2022 [<a href="https://www.youtube.com/watch?v=FNj8UScJmrk" target=&ldquo;blank&rdquo;>record</a>].
</p>
</li>
<li><p>UC San Diego, USA, Dec. 8, 2022 (Host: <a href="https://roseyu.com/" target=&ldquo;blank&rdquo;>Prof. Rose Yu</a>).
</p>
</li>
<li><p>IAA Research Summit, Johns Hopkins University, USA, Nov. 2022 [<a href="assets/offrl_iaa_22.pdf" target=&ldquo;blank&rdquo;>slides</a>].
</p>
</li>
<li><p>Ohio State University, USA, Jan. 2022 (Host: <a href="https://sites.google.com/view/yingbinliang/home" target=&ldquo;blank&rdquo;>Prof. Yingbin Liang</a> and <a href="http://newslab.ece.ohio-state.edu/home/" target=&ldquo;blank&rdquo;>Prof. Ness Shroff</a>).
</p>
</li>
<li><p>University of Arizona, USA, Dec. 2021 (Host: <a href="https://kwangsungjun.github.io/" target=&ldquo;blank&rdquo;>Prof. Kwang-Sung Jun</a>).
</p>
</li>
<li><p>Virginia Tech, USA, Nov. 2021 (Host: <a href="https://sites.google.com/site/thinhdoan210/home" target=&ldquo;blank&rdquo;>Prof. Thinh T. Doan</a>).
</p>
</li>
</ul>
<h2>For students </h2>
<ul>
<li><p><a href="https://github.com/thanhnguyentang/ML_Theory" target=&ldquo;blank&rdquo;>Learning materials for ML theory</a>
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
</div>
</div>
</div>
</body>
</html>
