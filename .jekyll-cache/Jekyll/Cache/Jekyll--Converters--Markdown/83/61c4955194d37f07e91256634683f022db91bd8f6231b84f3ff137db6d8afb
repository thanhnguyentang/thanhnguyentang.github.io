I"/<p><a href="/">[Back Home]</a></p>

<p>I have been thinking along these lines:</p>

<ul>
  <li><strong>Reinforcement Learning</strong>:
    <ul>
      <li><em>How can I apply approximate inference for efficient exploration in RL?</em></li>
      <li>Can multi-particle TD learning be a way for provably efficient RL?</li>
    </ul>
  </li>
  <li><strong>Information theory</strong>:
    <ul>
      <li>How can I use information theory to improve neural representation (e.g., to improve information flow in neural nets)?</li>
      <li>Wasserstein information bottleneck?</li>
      <li><em>Riemannian information bottleneck?</em> How to leverage Riemannian geometry of parameterization to improve VIB?</li>
      <li>Can I use information theory to improve variational inference?</li>
    </ul>
  </li>
  <li><strong>Approximate inference</strong>:
    <ul>
      <li>What is posterior mode collapse and how to attack this problem?</li>
      <li>How to tighten the ELBO in VAE?</li>
      <li>Inference in a manifold?</li>
    </ul>
  </li>
  <li><strong>Generative models</strong>:
    <ul>
      <li>A metric that can reflect the quality of generated images from GAN?</li>
      <li><em>Is mode jumping a thing and can we measure it by leveraging geometric properties in the discriminator?</em></li>
    </ul>
  </li>
</ul>
:ET