<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Thanh Nguyen-Tang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<div id="layout-content">
</br>
 <h1>Thanh Nguyen-Tang</h1>
 <img alt="tnt" src="profile_picture.png"
style="display:inline;margin:1px 15px 1px 1px;float:center" width="140"  id="profileimage">
<div id="subtitle">Postdoctoral Research Fellow <br /> 
<a href="https://www.cs.jhu.edu/" target=&ldquo;blank&rdquo;>Department of Computer Science</a> <br /> 
<a href="https://engineering.jhu.edu/" target=&ldquo;blank&rdquo;>Whiting School of Engineering</a> <br /> 
<a href="https://www.jhu.edu/" target=&ldquo;blank&rdquo;>Johns Hopkins University</a> <br /> 
Malone Hall 345, 3400 N Charles Street, Baltimore, MD 21218 <br /> 
<i>nguyent</i> at cs dot jhu dot edu / <i>thnguyentang</i> at gmail dot com <br /> 
[<a href="https://scholar.google.com/citations?hl=en&amp;user=UrTlMiwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=&ldquo;blank&rdquo;>Google Scholar</a>] [<a href="https://github.com/thanhnguyentang" target=&ldquo;blank&rdquo;>Github</a>]  [<a href="https://almostcompletenotes.wordpress.com/" target=&ldquo;blank&rdquo;>blog</a>] <br /><br />
</div>
<p><font color=red > *I'm on the 2024-2025 job market [<a href="preprints/research_statement.pdf">research statement</a>].</font>
</p>
<h2>Background</h2>
<p>I am currently a postdoc at Johns Hopkins University (with <a href="https://www.cs.jhu.edu/~raman/Home.html" target=&ldquo;blank&rdquo;>Raman Arora</a>). 
Prior to that, I did my PhD in Computer Science at <a href="https://a2i2.deakin.edu.au/publications/" target=&ldquo;blank&rdquo;>The Applied AI Institute</a>, Deakin University, Australia 
(Alfred Deakin Medal for Doctoral Theses). 
I did my M.Sc. in Computer Science at Ulsan National Institute of Science and Technology, South Korea. 
In my previous life, I studied Electronic and Communication Engineering (Talented Engineering Program) at Danang University of Science and Technology, Vietnam. 
</p>
<h2>Research Interest </h2>
<p><font color=red > &#8201;&mdash;&#8201;<i><b>Make the world an \(\epsilon\)-better place</b></i> </font>
</p>
<p>An overarching goal of my research is to establish <b>A</b>lgorithmic <b>F</b>oundations of <b>L</b>earning for modern <b>AI</b> (<b>AFLAI Lab</b>), 
with the vision of enabling next-generation AI with better <i>scalability</i>, <i>explainability</i>, and <i>transferability</i>. My approach emphasizes understanding learning through the lens of critical resources (e.g., data, and computation) and designing optimal algorithms that use these resources efficiently. My research agenda for the AFLAI Lab spans four main thrusts:
</p>
<ul>
<li><p><b>Transfer learning</b> (e.g., offline learning, multi-task/representation learning, federated learning, domain adaptation)
</p>
</li>
<li><p><b>Multi-agent learning</b> (e.g., policy regret minimization, equilibrium computation, mechanism design for learning agents)
</p>
</li>
<li><p><b>Trustworthy AI</b> (e.g., distributional/adversarial robustness, distributional learning, differential privacy)
</p>
</li>
<li><p><b>Large langugage models</b> (e.g., representation, optimization, and generalization aspects of transformers, emerging abilities such as
in-context learning and reasoning)
</p>
</li>
</ul>
<p><b>Keywords</b>: <i>learning</i>, <i>representation</i>, <i>optimization</i>, <i>computation</i>. 
</p>
<p>I welcome and appreciate <a href="https://forms.gle/LBPu7gNDi66VHnxV7" target=&ldquo;blank&rdquo;>anonymous feedback</a> from anyone on anything. 
</p>
<h2>Publications </h2>

<h3>2024 </h3>
<p>22.  
  <img alt="thinking" src="Felix_thinking.gif"
  style="display:inline;margin: 0px 0px -5px 0px;float:center" width="70"  id="thinking">
</p>
<p>21. Ragja Palakkadavath, Hung Le, <u>Thanh Nguyen-Tang</u>, Svetha Venkatesh, Sunil Gupta. <a href="https://openreview.net/pdf?id=3wL1tj3kqE" target=&ldquo;blank&rdquo;>Fair Domain Generalization with Heterogeneous Sensitive Attributes Across Domains</a>. WACV&rsquo;25.<br /> 
20. <u>Thanh Nguyen-Tang</u>, Raman Arora. <a href="preprints/19167_Learning_in_Markov_Games.pdf">Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms</a>. NeurIPS&rsquo;24.  <br />
19. Austin Watkins, <u>Thanh Nguyen-Tang</u>, Enayat Ullah, Raman Arora. <a href="preprints/19715_Adversarially_Robust_Mul.pdf">Adversarially Robust Multi-task Representation Learning</a>. NeurIPS&rsquo;24. <br />
18. Haque Ishfaq, <u>Thanh Nguyen-Tang</u>, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup. <a href="preprints/8601_Offline_Multitask_Represe.pdf">Offline Multitask Representation Learning for Reinforcement Learning</a>. NeurIPS&rsquo;24. <br />
17. <u>Thanh Nguyen-Tang</u>, Raman Arora. <a href="https://openreview.net/pdf?id=dYDPcx78tm" target=&ldquo;blank&rdquo;>On The Statistical Complexity of Offline Decision-Making</a>. International Conference on Machine Learning (<font color=red >ICML</font>), 2024. 
</p>
<h3>2023 </h3>
<p>16. Anh Tong, <u>Thanh Nguyen-Tang</u>, Dongeun Lee, Toan Tran, Jaesik Choi. <a href="https://arxiv.org/abs/2310.13369" target=&ldquo;blank&rdquo;>SigFormer: Signature Transformers for Deep Hedging</a>. 4th ACM International Conference on AI in Finance (<font color=red >ICAIF</font>), 2023 (<font color=red >Oral</font>). <br /> 
15. Anh Do, <u>Thanh Nguyen-Tang</u>, Raman Arora.  <a href="https://openreview.net/forum?id=7f6vH3mmhr" target=&ldquo;blank&rdquo;>Multi-Agent Learning with Heterogeneous Linear Contextual Bandits</a>. Advances in Neural Information Processing Systems (<font color=red >NeurIPS</font>), 2023. <br />
14. Austin Watkins, Enayat Ullah, <u>Thanh Nguyen-Tang</u>, Raman Arora. <a href="https://openreview.net/forum?id=gQ4h6WvME0" target=&ldquo;blank&rdquo;>Optimistic Rates for Multi-Task Representation Learning</a>. Advances in Neural Information Processing Systems (<font color=red >NeurIPS</font>), 2023. <br />
13. <u>Thanh Nguyen-Tang</u>, Raman Arora. <a href="https://openreview.net/forum?id=sdlh4gVOj8" target=&ldquo;blank&rdquo;>On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling and Beyond</a>. Advances in Neural Information Processing Systems (<font color=red >NeurIPS</font>), 2023. <br />
12.  Ragja Palakkadavath, <u>Thanh Nguyen-Tang</u>, Hung Le, Svetha Venkatesh, Sunil Gupta. <a href="https://openreview.net/pdf?id=Yl_4LpR_3Z" target=&ldquo;blank&rdquo;>Domain Generalization with Interpolation Robustness</a>. Asian Conference on Machine Learning (<font color=red >ACML</font>), 2023. <br />
11. Thong Bach, Anh Tong, Truong Son Hy, Vu Nguyen, <u>Thanh Nguyen-Tang</u>. <a href="https://openreview.net/forum?id=xWrtiJwJj5" target=&ldquo;blank&rdquo;>Global Contrastive Learning for Long-Tailed Classification</a>. Transactions on Machine Learning Research (<font color=red >TMLR</font>), 2023. <br />
10. A. Tuan Nguyen, <u>Thanh Nguyen-Tang</u>, Ser-Nam Lim, Philip Torr. <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2023_paper.html" target=&ldquo;blank&rdquo;>TIPI: Test Time Adaptation with Transformation Invariance</a>. IEEE/CVF Conference on Computer Vision and Pattern Recognition <font color=red >(CVPR)</font>, 2023.  <br />
9. <u>Thanh Nguyen-Tang</u>, Raman Arora. <a href="https://openreview.net/forum?id=WOquZTLCBO1" target=&ldquo;blank&rdquo;>VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation</a> 
International Conference on Learning Representations (<font color=red >ICLR</font>), 2023 (<font color=red >top 25% noble</font>).
[<a href="https://recorder-v3.slideslive.com/?share=80688&amp;s=a24d555f-edda-4210-a19e-4fded4165c62" target=&ldquo;blank&rdquo;>talk</a>] [<a href="assets/viper.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="https://github.com/thanhnguyentang/neural-offline-rl" target=&ldquo;blank&rdquo;>code</a>]  [<a href="notes/erratumiclr23.html" target=&ldquo;blank&rdquo;><b>ERRATUM</b></a>] <br />
8.  <u>Thanh Nguyen-Tang</u>, Ming Yin, Sunil Gupta, Svetha Venkatesh, Raman Arora. <a href="home">On Instance-Dependent Bounds for Offline Reinforcement Learning with Linear Function Approximation</a>.
AAAI Conference on Artificial Intelligence (<font color=red >AAAI</font>), 2023 [<a href="https://arxiv.org/abs/2211.13208" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="assets/aaai23-poster.pdf" target=&ldquo;blank&rdquo;>poster</a>] [<a href="assets/aaai23-slides-only.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="https://youtu.be/xUsMHodydO8" target=&ldquo;blank&rdquo;>video</a>].
</p>
<h3>2022   </h3>
<p>7. Anh Tong, <u>Thanh Nguyen-Tang</u>, Toan Tran, Jaesik Choi. <a href="https://openreview.net/pdf?id=lTZBRxm2q5" target=&ldquo;blank&rdquo;>Learning Fractional White Noises in Neural Stochastic Differential Equations</a>.
Advances in Neural Information Processing Systems (<font color=red >NeurIPS</font>), 2022. 
[<a href="https://github.com/anh-tong/fractional_neural_sde" target=&ldquo;blank&rdquo;>code</a>].  <br />
6. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, A.Tuan Nguyen, and Svetha Venkatesh. 
<a href="https://openreview.net/pdf?id=sPIFuucA3F" target=&ldquo;blank&rdquo;>Offline Neural Contextual Bandits:  Pessimism, Optimization and Generalization</a>.
International Conference on Learning Representations (<font color=red >ICLR</font>), 2022. 
[<a href="https://arxiv.org/abs/2111.13807" target=&ldquo;blank&rdquo;>arXiv</a>] 
[<a href="assets/poster_NeurIPSW21.pdf" target=&ldquo;blank&rdquo;>poster</a>]  
[<a href="assets/neuralcb_slides.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://github.com/thanhnguyentang/offline_neural_bandits" target=&ldquo;blank&rdquo;>code</a>]. <br />
5. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Hung Tran-The, Svetha Venkatesh. <a href="https://openreview.net/forum?id=LdEm0umNcv" target=&ldquo;blank&rdquo;>On Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks in Besov Spaces</a>. 
Transactions on Machine Learning Research (<font color=red >TMLR</font>), 2022, Workshop on RL Theory, ICML, 2021. [<a href="https://arxiv.org/abs/2103.06671" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://thanhnguyentang.github.io/assets/offrelu.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://www.youtube.com/watch?v=xLM5pondWY4" target=&ldquo;blank&rdquo;>talk</a>]. <br />
</p>
<h3>2021</h3>
<p>4. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Svetha Venkatesh. 
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17104" target=&ldquo;blank&rdquo;>Distributional Reinforcement Learning via Moment Matching</a>. 
AAAI Conference on Artificial Intelligence (<font color=red >AAAI</font>), 2021. 
[<a href="http://arxiv.org/abs/2007.12354" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/mmdrl" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://cutt.ly/fkkiAGm" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://cutt.ly/4kkiJZt" target=&ldquo;blank&rdquo;>poster</a>] [<a href="https://youtu.be/1fMqZZjy84E" target=&ldquo;blank&rdquo;>talk</a>].  
</p>
<h3>2020</h3>
<p>3. <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Huong Ha, Santu Rana, Svetha Venkatesh. 
<a href="http://proceedings.mlr.press/v108/nguyen20a.html" target=&ldquo;blank&rdquo;>Distributionally Robust Bayesian Quadrature Optimization</a>. 
International Conference on Artificial Intelligence and Statistics (<font color=red >AISTATS</font>), 2020. 
[<a href="https://arxiv.org/abs/2001.06814" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/drbqo" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://thanhnguyentang.github.io/assets/aistats20_drbqo.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://slideslive.com/38930124/" target=&ldquo;blank&rdquo;>talk</a>].  
</p>
<h3>2019</h3>
<p>2. Huong Ha, Santu Rana, Sunil Gupta, <u>Thanh Nguyen-Tang</u>, Hung Tran-The, Svetha Venkatesh.
<a href="https://papers.nips.cc/paper/9350-bayesian-optimization-with-unknown-search-space" target=&ldquo;blank&rdquo;>Bayesian Optimization with Unknown Search Space</a>.
Advances in Neural Information Processing Systems (<font color=red >NeurIPS</font>), 2019. 
[<a href="https://github.com/HuongHa12/BO_unknown_searchspace" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://postersession.ai/poster/bayesian-optimization-with-unknown-searc/" target=&ldquo;blank&rdquo;>poster</a>].<br />
1. <u>Thanh Nguyen-Tang</u>, Jaesik Choi. 
<a href="https://doi.org/10.3390/e21100976" target=&ldquo;blank&rdquo;>Markov Information Bottleneck to Improve Information Flow in Stochastic Neural Networks</a>. <font color=red >Entropy</font>, 2019 (Special Issue on Information Bottleneck: Theory and Applications in Deep Learning).   <br />
</p>
<h3>Preprints </h3>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Ming Yin, Masatoshi Uehara, Yu-Xiang Wang, Mengdi Wang, Raman Arora. <a href="https://openreview.net/pdf?id=WwCirclMvl" target=&ldquo;blank&rdquo;>Posterior Sampling via Langevin Monte Carlo for Offline Reinforcement Learning</a>. In OpenReview 2023.
</p>
</li>
<li><p>Nguyen Hung-Quang, Ngoc-Hieu Nguyen, The-Anh Ta, <u>Thanh Nguyen-Tang</u>, Kok-Seng Wong, Hoang Thanh-Tung,
and Khoa D Doan. <a href="https://arxiv.org/pdf/2407.10825" target=&ldquo;blank&rdquo;>Wicked oddities: Selectively poisoning for effective clean-label backdoor attacks</a>. In NeurIPS 2023
Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly, 2023. <br />
</p>
</li>
<li><p>Nguyen Ngoc-Hieu, Nguyen Hung-Quang, The-Anh Ta, <u>Thanh Nguyen-Tang</u>, Khoa D Doan, Hoang Thanh-Tung. <a href="https://arxiv.org/pdf/2306.14920" target=&ldquo;blank&rdquo;>A Cosine Similarity-based Method for Out-of-Distribution Detection</a>. In ArXiv 2023. <br /> 
</p>
</li>
<li><p>Mengyan Zhang, <u>Thanh Nguyen-Tang</u>, Fangzhao Wu, Zhenyu He, Xing Xie, Cheng Soon Ong. 
<a href="https://arxiv.org/pdf/2206.14648" target=&ldquo;blank&rdquo;>Two-Stage Neural Contextual Bandits for Adaptive Personalised Recommendation</a>. In Arxiv 2022.<br />
</p>
</li>
<li><p>Hung Tran-The, <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Santu Rana, and Svetha Venkatesh. <a href="https://arxiv.org/pdf/2107.11533" target=&ldquo;blank&rdquo;>Combining online learning
and offline learning for contextual bandits with deficient support</a>. In ArXiv 2021.
</p>
</li>
</ul>
<h2>Mentoring </h2>
<ul>
<li><p><a href="https://scholar.google.com/citations?user=cVTpiuoAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Ragja Palakkadavath</a> (PhD student at Deakin University, out-of-distribution generalization)  
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=yFLbTtkAAAAJ&amp;hl=en&amp;authuser=1" target=&ldquo;blank&rdquo;>Thong Bach</a> (independent researcher, self-supervised learning and domain adaptation)
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=aB4jrTIAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Anh Do</a> (PhD student at JHU, bandit/reinforcement learning)
</p>
</li>
<li><p><a href="https://scholar.google.com/citations?user=pSeyGjMAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Austin Watkins</a> (PhD student at JHU, transfer learning and robustness) 
</p>
</li>
</ul>
<h2>Teaching  </h2>
<ul>
<li><p>Co-instructor (with Raman Arora), Machine Learning: Advanced Topics: Foundations of Data-Driven Sequential Decision-Making Systems (CS 779), JHU, Spring 2024. 
</p>
</li>
<li><p>Teaching RL Theory in our JHU ML reading group, Summer/Fall 2023.  [<a href="other_pages/rl-notes.html" target=&ldquo;blank&rdquo;>notes</a>] 
</p>
</li>
<li><p>Guest lecturer (in bandits/reinforcement learning): Machine Learning (CS 475/675) Spring 2023, JHU. [<a href="other_pages/rl-lectures.html" target=&ldquo;blank&rdquo;>notes</a>] 
</p>
</li>
<li><p>Teaching Assistant: Statistical Machine Learning, Fall 2017, UNIST; Engineering Programming I/II, Spring 2016, UNIST; Various advanced mathematics and engineering courses, 2012-2016, Vietnam.  
</p>
</li>
</ul>
<p>* I participated in (and obtained a certificate of) Justice, Equity, Diversity, and Inclusion (JEDI) Training in the Classroom in March 2024 at JHU, as an effort to improve diversity in my future classes and research group. 
</p>
<h2>Selected Award/Honor   </h2>
<ul>
<li><p>Alfred Deakin Medal for Doctoral Theses (for the most outstanding theses), 2022.
</p>
</li>
</ul>
<h2>Independent Recognition </h2>
<ul>
<li><p>I am acknowledged in <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target=&ldquo;blank&rdquo;>Francis Bach's book, &ldquo;Learning Theory from First Principles&rdquo;</a>
</p>
</li>
<li><p>My AAAI&rsquo;21 paper is featured as an excercise in <a href="https://mitpress.mit.edu/9780262048019/distributional-reinforcement-learning/" target=&ldquo;blank&rdquo;>Bellemare, Dabney, and Rowland's book, &ldquo;Distributional Reinforcement Learning&rdquo;</a>
</p>
</li>
</ul>
<h2>Professional Service  </h2>
<p>Area Chair/Senior Program Committee
</p>
<ul>
<li><p>International Conference on Artificial Intelligence and Statistics (AISTATS) 2025
</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence (AAAI) 2025, 2024, 2023
</p>
</li>
</ul>
<p>Conference Reviewer/Program Committee
</p>
<ul>
<li><p>Neural Information Processing Systems (NeurIPS) 2024, 2023, 2022, 2021, 2020
</p>
</li>
<li><p>International Conference on Machine Learning (ICML) 2023, 2022, 2021
</p>
</li>
<li><p>International Conference on Learning Representations (ICLR) 2024, 2023, 2022, 2021 (outstanding reviewer award)
</p>
</li>
<li><p>AAAI Conference on Artificial Intelligence (AAAI) 2022, 2021 (top 25% reviewer)
</p>
</li>
<li><p>International Conference on Artificial Intelligence and Statistics (AISTATS) 2021
</p>
</li>
<li><p>Annual Learning for Dynamics &amp; Control Conference (L4DC) 2022
</p>
</li>
</ul>
<p>Coordinator
</p>
<ul>
<li><p>AAAI Conference on Artificial Intelligence (AAAI) 2023 (session chair for ML theory)
</p>
</li>
<li><p>International Conference on Machine Learning (ICML) 2022
</p>
</li>
<li><p>International Conference on Automated Machine Learning (AutoML) 2022
</p>
</li>
</ul>
<h2>Invited Talks </h2>
<ul>
<li><p>TrustML Young Scientist Seminars, RIKEN Japan, Aug. 01, 2023 [<a href="https://trustmlresearch.github.io/seminar-talks/index_Thanh_Nguyen.html" target=&ldquo;blank&rdquo;>post</a>] [<a href="assets/riken23.pdf">slides</a>] [<a href="https://www.youtube.com/watch?v=Z-r2XmxLAgk" target=&ldquo;blank&rdquo;>video</a>].
</p>
</li>
<li><p>VinAI, Vietnam, Jan. 13, 2023 [<a href="https://www.vinai.io/seminar-posts/provable-offline-reinforcement-learning-neural-function-approximation-randomization-and-sample-complexity/" target=&ldquo;blank&rdquo;>post</a>].
</p>
</li>
<li><p>FPT AI, Vietnam, Dec. 21, 2022 [<a href="https://www.youtube.com/watch?v=FNj8UScJmrk" target=&ldquo;blank&rdquo;>record</a>].
</p>
</li>
<li><p>UC San Diego, USA, Dec. 8, 2022 (Host: <a href="https://roseyu.com/" target=&ldquo;blank&rdquo;>Prof. Rose Yu</a>).
</p>
</li>
<li><p>IAA Research Summit, Johns Hopkins University, USA, Nov. 2022 [<a href="assets/offrl_iaa_22.pdf" target=&ldquo;blank&rdquo;>slides</a>].
</p>
</li>
<li><p>Ohio State University, USA, Jan. 2022 (Host: <a href="https://sites.google.com/view/yingbinliang/home" target=&ldquo;blank&rdquo;>Prof. Yingbin Liang</a> and <a href="http://newslab.ece.ohio-state.edu/home/" target=&ldquo;blank&rdquo;>Prof. Ness Shroff</a>).
</p>
</li>
<li><p>University of Arizona, USA, Dec. 2021 (Host: <a href="https://kwangsungjun.github.io/" target=&ldquo;blank&rdquo;>Prof. Kwang-Sung Jun</a>).
</p>
</li>
<li><p>Virginia Tech, USA, Nov. 2021 (Host: <a href="https://sites.google.com/site/thinhdoan210/home" target=&ldquo;blank&rdquo;>Prof. Thinh T. Doan</a>).</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
</div>
</div>
</div>
</body>
</html>
