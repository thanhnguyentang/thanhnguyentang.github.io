---
layout: post
title: My research directions  
---  
[[Back Home]](/) 


* **Reinforcement Learning**: 
    * *How can I apply approximate inference for efficient exploration in RL?* 
    * Can multi-particle TD learning be a way for provably efficient RL? 

* **Information theory**: 
    * How can I use information theory to improve neural representation (e.g., to improve information flow in neural nets)? 
    * Wasserstein information bottleneck? 
    * *Riemannian information bottleneck?* How to leverage Riemannian geometry of parameterization to improve VIB?
    * Can I use information theory to improve variational inference? 

* **Approximate inference**: 
    * What is posterior mode collapse and how to attack this problem? 
    * How to tighten the ELBO in VAE? 
    * Inference in a manifold? 

* **Generative models**: 
    * A metric that can reflect the quality of generated images from GAN? 
    * *Is mode jumping a thing and can we measure it by leveraging geometric properties in the discriminator?*
