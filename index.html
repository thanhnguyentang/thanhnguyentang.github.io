
<html>

    <head>
		 <script type="text/x-mathjax-config">
			   MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		 </script>
		 <script type="text/javascript"
	     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		 </script>
        <meta charset="UTF-8"> 
        <link rel="stylesheet" href='css/style3.css'>
        
        <SCRIPT TYPE="text/javascript"> 
        function popup(mylink, windowname)
        {
            if (! window.focus)
                return true;
            var href;
            if (typeof(mylink) == 'string')
                href=mylink;
            else
                href=mylink.href;
            window.open(href, windowname, 'width=750,height=300,scrollbars=yes');
                return false;
        }
            //-->
        </SCRIPT> 
        <title>thanhnguyentang</title>
    </head>
    
    <style type="text/css">
    .journal { color: #117711; }
    </style>
    
    <body>
            <div>
            <!--
            <center>
            <img src="images/banner.jpg" width=100%>
            </center>
            -->
            
            <div style='float:right; font-size:13px; width:254; padding:5px; margin:10px; margin-right:-5px; border-left:0px solid #000000'>
            
                <center>
                <b>Upcoming Events</b>
                </center>
            
                <div class='event' style='background-color:#EECCAA'>   
                    <ul style="padding-left: 14px;">
                        <li>
                            <b><strike>03-05/06/20</strike> Aug. 26-28, 2020:</b> 
                            <a href="https://www.aistats.org/"><b>AISTATS</b> 2020</a> in Palermo, Sicily, Italy (postponed due to COVID-19).
                        </li>
                        
                        <li>
                            <b>Aug. 31-Sep. 4, 2020:</b>
                            Theory of RL <a href="https://simons.berkeley.edu/workshops/rl-2020-bc">BootCamp</a> by Simons Institute, UC Berkeley. 
                        </li>

                        <li>
                            <b>Oct. 19-23, 2020:</b>
                            Concentration of Measure Phenomena <a href="https://simons.berkeley.edu/workshops/hd-2020-2">workshop</a> by Simons Institute, UC Berkeley.
                        </li>

                        <li>
                            <b>Nov. 16-20, 2020:</b> 
                            Learning and Testing in High Dimensions <a href="https://simons.berkeley.edu/workshops/hd-2020-3">workshop</a> by Simons Institute, UC Berkeley.
                        </li>
                    </ul>                            
    
                    
                </div>
                
                <center>
                    <b>Past Events</b>
                    </center>
                
                    <div class='event' style='background-color:#F0DEDA'>
                        <ul style="padding-left: 14px;">
                            <li>
                                <b><font color="#000000">Jun. 28-Jul. 10, 2020:</font></b> 
                                <a href="http://mlss.tuebingen.mpg.de/2020/index.html">MLSS 2020</a> at MPI-IS, Tübingen, Germany (virtually due to COVID-19). 
                            </li>

                            <li>
                                <b><font color="#000000">Apr. 26-30, 2020:</font></b>
                                What an (virtual) experience with the first-ever fully-virtual conference <a href="https://iclr.cc/">ICLR 2020</a> (formerly Addis Ababa, Ethiopia). 
                            </li>
                        </ul>
                                      

     
                    </div>
                
                <center>
                    <b>News</b>
                </center>
                
                <div class='event'> 

                    <ul style="padding-left: 14px;">
                        <li>
                            <b><font color="#000000">Apr. 20, 2020: </font></b>
                            I have been accepted to <a href="http://mlss.tuebingen.mpg.de/2020/index.html">MLSS 2020</a> at the Max Planck Institute for Intelligent Systems, Tübingen, Germany (acceptance rate: 13.84%). 
                            Check out a list of amazing speakers <a href="http://mlss.tuebingen.mpg.de/2020/speakers.html">here</a>. 
                            Check out my note of MLSS20 <a href="https://thanhnguyentang.github.io/blogs/mlss20.pdf">here</a>.
                        </li>

                        <li>
                            <b><font color="#000000">Jan. 07, 2020:</font></b>
                            One paper 
                            has been accepted to  <a href="https://www.aistats.org/"><b>AISTATS</b> 2020</a>.
                        </li>

                        <li>
                            <b><font color="#000000">Sep. 30, 2019:</font></b> 
                            One manuscript on has been accepted to 
                            <a href="https://www.mdpi.com/journal/entropy">Entropy journal</a>. 
                            Special thanks to <a href="https://scholar.google.com/citations?user=LzQ2jAwAAAAJ&hl=en">Bernhard C. Geiger</a>
                            for inviting us to contribute to Entropy.
                        </li>

                        <li>
                            <b><font color="#000000">Sep. 04, 2019:</font></b> 
                            Our paper has been accepted to 
                            
                            <a href="https://neurips.cc/Conferences/2019/AcceptedPapersInitial"><b>NeurIPS</b> 2019</a>, congrats
                            <a href="https://scholar.google.com.au/citations?user=Uf3o7_UAAAAJ&hl=en">Huong Ha</a>!
                        </li>

                        <li>
                            <b><font color="#000000">Jan. 16, 2019:</font></b> I started Ph.D. with <a href="https://a2i2.deakin.edu.au/">A2I2@Deakin</a> (Australia).
                        </li>

                        <li>
                            <b><font color="#000000">Nov. 20, 2017:</font></b> I defended my Master with A/Prof. J. Choi, A/Prof. <a href="http://bmipl.unist.ac.kr/">S. Y. Chun</a> and 
                            Assistant Prof. <a href="https://junmoony.github.io/">J. Moon</a> (now at University of Seoul) at UNIST.
                        </li>

                    </ul>
                     
                </div>
            
            </div>
                    <!--
                    
                    <div style="width:10%;float:left">
                    <img alt="Thanh T. NGUYEN, T.T. Nguyen, Thanh Nguyen" src="Profile.jpg" style="display:inline;margin:5px 10px 0px 0px;float:left" width="150">
                    <a href="/personal/images" target="_blank"><font size="1">more images</font></a>
                    </div>

                -->
                <br> 
                <br>
                <img alt="Thanh T. NGUYEN, T.T. Nguyen, Thanh Nguyen" src="images/IMG_4751.jpeg" 
                style="display:inline;margin:5px 10px 0px 0px;float:left" width="120">
                <b>Thanh Tang NGUYEN</b>  <br>
                <a href="https://a2i2.deakin.edu.au/" target="_blank">Applied AI Institute (A2I2)</a><br> 
                <a href="http://www.deakin.edu.au/" target="_blank">Deakin University</a><br> 
                75 Pigdons Rd, Highton VIC 3216, Australia<br> 
                Email: thanhnt [AT] deakin.edu.au / nguyent2792 [AT] gmail.com <br>
                <!--<img src="images/Email.png" alt="Email" height="42"><br>-->
                <br>
                [ <a href="/assets/NGUYEN_CV.pdf" target="_blank">CV</a> | 
                    <a href="https://github.com/thanhnguyentang" target="_blank">Github</a> | 
                    <a href="/blogs/post">Notes</a> | 
                    <a href="https://scholar.google.com/citations?hl=en&user=UrTlMiwAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a> | 
                    <a href="https://www.researchgate.net/profile/Thanh_Nguyen298" target="_blank">ResearchGate</a> |  
                    <a href="https://twitter.com/thanhnguyentang" target="_blank">Twitter</a> |
                    <a href="/photos/main">Photos</a> ]
                    

            <p>
            I am currently a PhD candidate at the Applied AI Institute of Deakin University 
            with A/Prof. <a href="https://scholar.google.com.au/citations?user=bXeL2t8AAAAJ&hl=en">Sunil Gupta</a>, and 
                                <!-- A/Prof. <a href="https://scholar.google.com.au/citations?user=S9PwnMYAAAAJ&hl=en">Santu Rana</a>,  -->
                                <!-- A/Prof. <a href="https://scholar.google.com.au/citations?user=zvspVLwAAAAJ&hl=en">Truyen Tran</a>, and  -->
                                Alfred Deakin Prof. <a href="https://scholar.google.com.au/citations?user=AEkRUQcAAAAJ&hl=en">Svetha Venkatesh</a>. 
            Prior to that, I was working as a researcher with A/Prof. <a href="http://sail.unist.ac.kr/members/jaesik/">Jaesik Choi</a> 
            (now at <a href="https://sites.google.com/a/kaist.edu/song-chong/kaist-graduate-school-of-ai">KAIST</a>) at <a href="http://sail.unist.ac.kr/" target="_blank">SAIL@UNIST</a> 
            where I also obtained a master degree in 2018. Before that, I got my B.Eng. in the Center of Excellence, Danang University of Science and Technology in 2015.  
            <!-- <br>
            My ha-index is 70.  -->
            <br><br>
            <!-- My research interest is in the ideas that involve distribution discrepancy and their application specifically in reinforcement learning.  
            I am also strongly motivated by empirical AI problems of designing a lifelong, continual-learning, and self-improving agent that becomes more general and smarter under minimal supervision 
            (sufficiently general AI).  -->

            
            <!-- One day, such a agent can become general enough that it can help us free our labor in many repeated tasks and gives us more freedom to do more great things. 
            I am still skeptical of a truly general AI, but I believe that we can make it a lots more general than it is today, and 
            <i>sufficiently general </i> AI is the future of AI that I believe.  -->

            <!-- My underlying mission is to make Machine Learning and Artificial Intelligence (MLAI) more fun, more accessible, and more powerful.
            My broad interests are in the ideas that involve playing with distributions in an online manner and are strongly inspired by important emprical problems.   -->
            <!-- My broad interests are in the ideas that involve sequential decision making under uncertainty, probability, information theory and neural networks  -->
            <!-- with empirical motivations.  -->

            <!-- My research goal is to develop more theoretically principled algorithms with deeper understanding and stronger empirical performance for important AI problems. 
            My research interests lie in the algorithmic, foundational and empirical dimensions at the intersection of machine learning, optimization, statistics and information theory; particularly: -->
            <!-- Currently, I mainly focus on optimization and exploration perspective in reinforcement learning, and a bit on uncertainty and generalization in deep neural networks.   -->
            <!-- I study the mathematical foundations of machine intelligence via information geometry, statistical inference and optimization (possibly on a manifold structure) 
            with an emphasis on designing geometrically effective, statistically efficient and robust, and scalable inference algorithms. 
            Currently, I focus on particle-based variational inference (rooted in optimal transport and information bottleneck) to study generalization and robustness in reinforcement learning and neural networks.  -->
            <!-- Some keywords for my interest span:  -->
            In general, I am interested in topics in inference, learning, and dynamics for modern machine learning, 
            with the goal of understanding mathematical foundations and designing new efficient algorithms of important problems arising from modern practical settings. 
            Specifically, I currently focus on statistical distributional estimations in reinforcement learning and generalization in neural networks via information geometry. 

            <!-- Currently, I focus on reinforcement learning and deep neural networks in both standard and distributional shift (or other causal interventions) settings; -->
            <!-- specifically in the aspects of representation learning (e.g., via self-supervised learning or information-theoretic perspectives) and information-geometric algorithms (e.g., via probability divergences and distributional optimization) in this domain.   -->
            
            <!-- I focus on representation learning and information-geometric algorithms for reinforcement learning and deep neural networks: 
            (i) representation learning: under distributional shift or other causal interventions (via self-supervised learning or information-theoretic perspectives), 
            (ii) information-geometric algorithms: to best leverage the geometry of parameterization (e.g., via probability divergences and distributional optimization).  -->
          
            <!-- I study representation learning ;  -->
            <!-- information-geometric algorithms  -->

            <!-- Specifically I am interested in reinforcement learning, computational statistics, distributional optimization and representation learning -->
            <!-- I'm interested in the intersection of machine learning with statistics, information theory and optimization;  -->
            <!-- with an emphasis on designing (i) novel learning algorithms that are geometrically effective, statistically efficient and robust, and scalable; -->
            <!-- (ii) robust neural representations .  -->
            
            <!-- In particular, I study variational inference and geometric insights of reinforcement learning and neural networks with the goal of improving and understanding their generalization and robustness. 
            In addition, I'm also interested in robust representation learning via self-supervised learning and information-theoretic perspectives.  -->
            
            <!-- ; in particular, reformulating a learning problem as statistical inference and optimization (possibly on a manifold) 
            for  geometrically effective, statistically efficient, and scalable learning algorithms.  
            Currently, I focus on statistical efficiency and robustness in reinforcement learning via geometry (and geometry itself).  -->
            
            <!-- My research goal is to understand machine intelligence from a computational perspective; to this end, I study inference, optimization and statistics.  -->
            <!-- Currently I focus on statistical inference and optimization on the probability distribution space (endowed with e.g. Wasserstein metric) -->
            <!-- and its application to robustness and stability in reinforcement learning. Keywords for my research interests:  -->
            <!-- <ul>
                <li>Reinforcement learning;</li> 
                <li>Approximate inference; </li>
                <li>Optimal transport and manifold structures;</li>
                <li>Optimization;</li>
                <li>Information bottleneck.</li>
                <!-- <li>Probabilistic modelling and inference; </li>  -->
                <!-- <li>GANs.</li> -->
                <!-- <li>Information Theory; </li>   -->
                <!-- <li>Uncertainty and generalization in deep (stochastic) neural networks. </li> -->
                <!-- <li>Applications: Neural Reading Comprehension and Medical Imaging.</li> -->

            <!-- </ul>  -->
    


            <!-- <br><br> -->
            
            <!-- <b>Keywords</b>: Reinforcement Learning; Distributional Learning; Variational Bayes; Uncertainty; Information Theory; Generalization in DNNs.  -->
            
            <!-- I identify my works to ICML, NeurIPS, AISTATS, AAAI, ICLR and Transactions on Information Theory.  -->



            <!-- I am interested in leveraging and developing principled approaches rooted in optimization, statistics and information theory to attack empirical AI problems. My current research interests: -->
		<!-- <ul>
            <li>Reinforcement Learning</li> 
            <li>Optimal Transport</li> 
            <li>Bayesian Statistics</li>
			<li>Variational Inference</li> posterior collapse problem
            <li>Information Theory</li> information bottleneck

        </ul>  -->
            <!--
            My long-term research effort is to seek for general computational principles underlying so-called <i>intelligence</i>. I believe that at the core of intelligence is the ability to <i>learn</i> under various <i>learning paradigms</i>. By learning, I mean the ability to generalize from past experience without exhaustedly experiencing all the intractable search space. By learning paradigms, I refer to <i>incremental</i> learning, i.e., to learn incrementally on a sample or episode or sequential basis (which is beneficial for scalability), either from interactions (as in reinforcement learning), or from external supervision (as in supervised learning), or for reconstruction (as in unsupervised learning). Currently, I am focusing on reinforcement learning, information-theoretic deep learning, and Bayesian statistics with vast applications in NLP, computer vision and healthcare. In particular, I am specially interested in the trade-off aspect of <i>learning</i> such as exploration-exploitation trade-offs in reinforcement learning, compression-relevance trade-offs in information bottleneck, and explainability-performance in explainable artificial intelligence. <br> <br>

            I avocate the "think-globally-act-locally" philosophy, promoting doing research in important Machine Learning and Artificial Intelligence problems by the means of <i>incrementality</i>, <i>simplicity</i> and <i>transparency</i>. 

            </p>

            <h2> Education</h2>
            <ul>
                <li> 2016/03-2018/02: M.Sc. in Computer Science and Engineering <br> <a href="http://unist.ac.kr/" target="_blank">
                Ulsan National Institute of Science and Technology (UNIST)</a>, Korea (GPA: 4.3 / 4.0)<br>
                </li>
                <li> 2010/09-2015/07: B.Sc. in Electronic and Communication Engineering <br> <a href="http://dut.udn.vn/EN" 
                target="_blank">Danang University of Science and Technology</a>, Vietnam (Valedictorian)<br>
                </li>
            </ul>

            <h2>Experience</h2>

            </li>

            <li>2016/03-2018/03: Research Assistant and Teaching Assistant at <a href="http://sail.unist.ac.kr/" target="_blank">Statistical Artificial Intelligence Lab</a>, UNIST

            </li>
            </ul>

            -->
            <h2>Publications</h2>
                <ul>
                <li><b>TT Nguyen</b>, S. Gupta, H. Ha, S. Rana, and S.Venkatesh <br>
                    <font color="blue">Distributionally Robust Bayesian Quadrature Optimization</font>. [<a href="https://arxiv.org/abs/2001.06814">arXiv</a> | <a href="http://proceedings.mlr.press/v108/nguyen20a.html">link</a> | <a href="https://github.com/thanhnguyentang/drbqo">code</a> | <a href="assets/aistats20_drbqo.pdf">slides</a> | <a href="https://slideslive.com/38930124/">talk</a>] <br> 
                            
                    <span class="journal"> Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (<b>AISTATS</b>), Palermo, Italy, <b>2020</b>.
                    
                    </span>
                
                </li>

                    <li><b>TT Nguyen</b>, and J. Choi<br>
                    <font color="blue">Markov Information Bottleneck to Improve Information Flow in Stochastic Neural Networks</font>.
                [<a href ="https://doi.org/10.3390/e21100976">DOI</a> | <a href="https://github.com/thanhnguyentang/pib">code</a> | <a href="bibtex/nguyen-pib17.txt" onclick="return popup(this, 'bibtex')">bibtex</a>]<br> 
                    <span class="journal"><b>Entropy</b>, 21(10), 976, <b>2019</b> (in <a href="https://www.mdpi.com/journal/entropy/special_issues/information_theoretic_computational_intelligence">
                        <b>Information Bottleneck: Theory and Applications in Deep Learning</b></a>).</span> </li>

                    <li>H. Ha, S. Rana, S. Gupta, <b>TT Nguyen</b>, H. Tran-The, and S. Venkatesh
                        <br>
                        <font color="blue">Bayesian Optimization with Unknown Search Space</font>. [<a href="https://papers.nips.cc/paper/9350-bayesian-optimization-with-unknown-search-space">link</a> | <a href="https://postersession.ai/poster/bayesian-optimization-with-unknown-searc/">poster</a> | <a href="https://github.com/HuongHa12/BO_unknown_searchspace">code</a> | <a href="bibtex/huong_neurips19.txt" onclick="return popup(this, 'bibtex')">bibtex</a>]<br>
                        <span class="journal">Proceedings of the Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 32, Vancouver, BC, Canada, 8–14 December, <b>2019</b>.</span>
                        </li>
                    <li><b>TT Nguyen</b> <br>
                        <font color="blue">Parametric Information Bottleneck to Optimize Stochastic Neural Networks</font>. [<a href="http://scholarworks.unist.ac.kr/handle/201301/23561">link</a> | <a href="/assets/pib_slide_v4.pdf">slides</a> | <a href="https://www.youtube.com/watch?v=md9qV4Hrgbo&t=378s">talk</a> | <a href="bibtex/nguyen-master18.txt" onclick="return popup(this, 'bibtex')">bibtex</a>]
                        <br><span class="journal">Graduate School of UNIST, <b>Master Thesis</b>, <b>2018</b>.</span> </li>
                    <li><b>TT Nguyen</b>, and J. Choi <br>
                        <font color="blue">Parametric Information Bottleneck to Optimize Stochastic Neural Networks</font>. [<a href="http://www.kiise.or.kr/data/pdf/PACS2017_Proceedings.pdf">link</a>] (<b>Best Poster Award</b>) <br>
                        <span class="journal"> Proceedings of the International Symposium on Perception, Action and Cognitive Systems (<b>PACS</b>), p. 23-30, Seoul, Korea, <b>2017</b>. </span>
                        <!-- [<a href="http://www.kiise.or.kr/conference/PACS/2017/" target="_blank">Best Poster Award</a>] -->
                    </li>
    
                </ul>
            <h4>Preprints</h4>
            <ul>
                 <li><b>TT Nguyen</b>, S. Gupta, and S.Venkatesh. <br> 
                    <font color="blue">Moment Matching Reinforcement Learning</font>. [ 
                    <a href="http://arxiv.org/abs/2007.12354">arXiv</a> 
                    | <a href="https://docs.google.com/presentation/d/1wS0B39CV08emqQD8Fkio-e1GJPhKco2qI9WVQo2gslc/edit#slide=id.p">slides</a> 
                    | <a href="https://youtu.be/o7MvhoFpWRM">talk</a> ]
                    <br>
                    <span class="journal">Under review, <b>2020</b>.</span></li> 
             </ul>

             <h2>Poster presentations</h2> 
             <ul>
                <li><b>Aug. 26-28, 2020:</b> Distributionally Robust Bayesian Quadrature Optimization @ <a href="https://www.aistats.org/">AISTATS 2020</a> </li>
                <li><b>Jun. 28-Jul. 10, 2020:</b> Moment Matching Reinforcement Learning @ <a href="http://mlss.tuebingen.mpg.de/2020/index.html">MLSS 2020</a> </li>
             </ul>

            <h2>Others</h2>
                <b>Service</b> 
                <ul>
                    <li> 
                        Invited reviewer:  ICLR (2021), NeurIPS (2020), IJCNN (2020). 
                    </li>

                    <!-- ; support reviewer: ICLR (2020), AISTATS (2018) -->

                </ul>
                <b>Teaching</b> 
                <ul>
                    <li>
                        <a href="http://sail.unist.ac.kr/">UNIST</a>: Advanced Machine Learning (TA, 2017);  Engineer Programming (TA, 2016).
                    </li>
                    <li>
                        <a href="http://fast.dut.udn.vn/en/">CoE@DUT</a>: Machine Learning (TA, 2015); Digital Signal Processing (TA, 2014); Advanced Calculus and Algebra (TA, 2011-2015). 
                    </li>
                </ul>
            
                <b> Past projects </b>
                <ul>
                    <!-- <li><a href="https://www.researchgate.net/project/Neural-Reading-Comprehension">Neural Reading Comprehension in SQuAD 2.0</a><br><span class="journal">2018-</span></li>
                    <li><a href="https://www.researchgate.net/project/Information-Theoretic-Interpretation-of-Deep-Neural-Networks">Information-Theoretic Interpretation of Deep Neural Networks</a><br><span class="journal">2017-</span></li> -->
                    <li>Neural reading comprehension (collaborators: <a href="https://scholar.google.co.kr/citations?user=N9ZM-CIAAAAJ&hl=ko">S. Kwon</a>, J.C.)
                        <br>
                        <span class="journal"> <a href="https://rajpurkar.github.io/SQuAD-explorer/">Stanford question answering dataset</a><b>, 2018</b>.</span> </li>
                    </li>

                    <li>Deep Learning based Real-Time Pedestrian Detection for CCTV 
                        (collaborators: <a href="https://scholar.google.com/citations?user=ZKhDYmUAAAAJ&hl=en">R. Lima</a>, and J.C.). [ <a href="http://saildemo.unist.ac.kr/pedestrian_detection/">demo</a> ]  <br>
                        <span class="journal">NIPA project, <b>2016</b>.</span> </li>

                    <li>Lesion Dermoscopic Feature Segmentation (ranked 1st) and Lesion Segmentation (ranked 8th) 
                        (collaborators: <a href="https://github.com/LieCOS">J. Ju</a>, 
                        <a href="https://haebeom-lee.github.io/">H. Lee</a>, 
                        <a href="http://bmipl.unist.ac.kr/">S.Y. Chun</a>, and J.C.). [ <a href="https://github.com/thanhnguyentang/melanoma_tutorial">code</a> | 
                         <a href="https://seyoungchun.wordpress.com/2016/04/15/posters-and-challenge-results-at-the-2016-international-symposium-on-biomedical-imaging-isbi/">BMIPL News</a> 
                         | <a href="http://sail.unist.ac.kr/ieee-isbi-2016-challenge-rank-1st-and-3rd-in-skin-cancer-classification-challenges/">SAIL News</a> ]<br>
                        
                        <span class="journal">ISBI 2016: Skin Lesion Analysis Towards Melanoma Detection, <b>2016</b>.</span> 
                    </li>

                </ul>
            
            <!--
                <h2> Activities </h2>
                <ul>
                    <li>
                    2018/10/12: Attended <a href="http://xai.unist.ac.kr/Symposium/2018/" target="_blank">International Explainable AI Symposium 2018</a>, Seoul, Korea.
                    </li>
                    <li>
                    2017/11/15-17: Attended <a href="http://www.acml-conf.org/2017/" target="_blank">the Asian Conference on Machine Learning (ACML) 2017</a>, Seoul, Korea.
                    </li>

                    <li>
                    2017/11/02-03: Attended <a href="http://www.kiise.or.kr/conference/PACS/2017/" target="_blank">the International Symposium on Perception, Action, and Cognitive Systems (PACS)</a>, Seoul, Korea.
                    </li>

                    <li>
                    2016/06/02-03: Attended <a href="http://mlcenter.postech.ac.kr/ml_symposium_2016_program" target="_blank">the First Korea-Japan Machine Learning Symposium</a>, Seoul, Korea.
                    </li>
                </ul>
            -->
            <!--
                <h2>Highlight</h2>
            
                <center>
                    <div style='display:inline-block;vertical-align:top;margin:10px;width:200px;color:#555555'>
                    <a alt="Fix2Att" href="/assets/[AML]relevant-from-fixations.pdf" target="_blank"><img src="assets/Ik3nhT4.gif" width=200px height=150px style="border:solid 2px #AAAAAA"></a><br><b>Attention from Fixations:</b> The model learns to attend to the salient regions in the videos from eye fixations. 
                    </div>
                    <div style='display:inline-block;vertical-align:top;margin:10px;width:200px;color:#555555'>
                    <a alt="PedestrainDetection" href="http://saildemo.unist.ac.kr/pedestrian_detection/" target="_blank"><img src="images/nipa_model.png" width=200px height=150px style="border:solid 2px #AAAAAA"></a><br><b>Pedestrian Detection:</b> The region proposal network directly predicts boxes of various scales at each position in an image. 
                    </div>
                    <div style='display:inline-block;vertical-align:top;margin:10px;width:200px;color:#555555'>
                    <a alt="PIB" href="https://arxiv.org/pdf/1712.01272v4.pdf" target="_blank"><img src="images/pib_model.png" width=200px height=150px style="border:solid 2px #AAAAAA"></a><br><b>PIB:</b> Each layer in DNNs is considered as a bottleneck that splits the architecture into an encoder and a variational relevance decoder. Compression and relevance are then induced at each bottleneck.
                    </div>
                    <div style='display:inline-block;vertical-align:top;margin:10px;width:200px;color:#555555'>
                    <a alt="Squad" href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank"><img src="images/squad20.png" width=200px height=150px style="border:solid 2px #AAAAAA"></a><br><b>Reading Comprehension:</b> The network answers to a question relevant to a context by selecting a span in the context. The question might or might not be answerable. 
                    </div>
                </center>
                -->
                
            
                
            
            
            
            <!--
            <div>
            <a href="https://info.flagcounter.com/btag"><img src="https://s05.flagcounter.com/count/btag/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>   
            </div>
            -->

        <i><font size="1">Last updated: 2020. </font></i> <br>
        <i><font size="1"> Built with <a href="https://jekyllrb.com/">Jekyll</a> | Inspired by <a href="http://heatmapping.org/" target="_blank">heatmapping</a>.</font></i><br><br>
        <!-- <i><font size="1">Every new day is blessed; one idea, one experiment, one proof at a time.</font></i> -->
    </div>
            <!--<p><font size="4" color="red">"Stand on the shoulders of giants"</font></p> -->
    </body>
</html>
    
