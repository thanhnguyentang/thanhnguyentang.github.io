<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Thanh Nguyen-Tang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<div id="layout-content">
</br>
 <h1>Thanh Nguyen-Tang</h1>
 <img alt="tnt" src="profile_pic.jpg"
style="display:inline;margin:5px 10px 0px 0px;float:center" width="163" id="profileimage">
<div id="subtitle">Postdoctoral Research Fellow <br />
<a href="https://www.cs.jhu.edu/" target=&ldquo;blank&rdquo;>Department of Computer Science</a> <br /> 
<a href="https://engineering.jhu.edu/" target=&ldquo;blank&rdquo;>Whiting School of Engineering</a> <br /> 
<a href="https://www.jhu.edu/" target=&ldquo;blank&rdquo;>Johns Hopkins University</a> <br /> <br />
Malone Hall 331, 3400 N Charles Street, Baltimore, MD 21218 <br /> <br /> 
<u>Email</u> <i>nguyent</i> at cs dot jhu dot edu / <i>thnguyentang</i> at gmail dot com <br /> <br />
<u>Profiles</u> <a href="https://scholar.google.com/citations?hl=en&amp;user=UrTlMiwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=&ldquo;blank&rdquo;>Scholar</a>, 
<a href="https://github.com/thanhnguyentang" target=&ldquo;blank&rdquo;>Github</a>
</div>
<h2>About Me  </h2>
<p>I am a Postdoctoral Research Fellow in Department of Computer Science at Johns Hopkins University, 
working with <a href="https://www.cs.jhu.edu/~raman/Home.html" target=&ldquo;blank&rdquo;>Raman Arora</a>. I was an Associate Research Fellow at <a href="https://a2i2.deakin.edu.au/publications/" target=&ldquo;blank&rdquo;>the Applied AI Institute</a>, Deakin University (Australia) in July 2021-June 2022 
and completed my PhD there in Feb 2022. I did my Master in Computer Science and Engineering at <a href="http://sailab.kaist.ac.kr/publications/" target=&ldquo;blank&rdquo;>the Statistical AI Lab</a>, Ulsan National Institute of Science and Technology (South Korea) in 2018. 
</p>
<h2>Research Interests </h2>
<p>I am studying the algorithmic and theoretical foundations of <i>modern</i> machine learning with a focus on the following pillars: <i>statistical efficiency</i>, <i>computational efficiency</i>, and <i>trustworthiness</i>, with topics: 
</p>
<ul>
<li><p>Reinforcement learning theory 
</p>
</li>
<li><p>(Deep) learning theory 
</p>
</li>
<li><p>Trustworthy machine learning (robust adversarial learning, differential privacy and OOD generalization)
</p>
</li>
<li><p>Representation (multi-task, federated, collaborative) learning 
</p>
</li>
<li><p>Probabilistic learning 
</p>
</li>
</ul>
<h2>Recent News   </h2>
<ul>
<li><p>02/27/2023: One paper accepted to CVPR&rsquo;23 (acceptance rate: 25.78%).  
</p>
</li>
<li><p>Jan. 20, 2023: One paper (top 25-percent noble) accepted to ICLR&rsquo;23 (acceptance rate: 31.8%).
</p>
</li>
<li><p>Dec. 9, 2022: One paper accepted to TMLR. 
</p>
</li>
<li><p>Nov. 19, 2022: One paper accepted to AAAI, 2023 (acceptance rate: 19.6%). 
</p>
</li>
<li><p>Oct. 30, 2022: I was acknowledged in Francis Bach's <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target=&ldquo;blank&rdquo;>&ldquo;Learning Theory from First Principles&rdquo;</a>.
</p>
</li>
<li><p>Sep. 14, 2022: One paper accepted to NeurIPS, 2022 (acceptance rate: 25.6%). 
</p>
</li>
<li><p>Jan. 21, 2022: One paper accepted to ICLR, 2022 (acceptance rate: 32.26%). 
</p>
</li>
<li><p>May 20, 2021: Accepted to the Deep Learning Theory Summer School at Princeton (acceptance rate: 180/500 = 36%).
</p>
</li>
</ul>
<h2>Publications  </h2>
<h3>2023  </h3>
<ul>
<li><p><b></b><b>On Langevin Posterior Sampling for Offline Reinforcement Learning</b><b></b><br /> 
<u>Thanh Nguyen-Tang</u>, Ming Yin, Masatoshi Uehara, Yu-Xiang Wang, Raman Arora<br />
Under review, 2023
</p>
</li>
</ul>
<ul>
<li><p><b></b><b>TIPI: Test Time Adaptation with Transformation Invariance</b><b></b><br /> 
A. Tuan Nguyen, <u>Thanh Nguyen-Tang</u>, Ser-Nam Lim, Philip Torr <br />
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<font color=red ><b></b><b>CVPR</b><b></b></font>), 2023 [<a href="https://atuannguyen.com/assets/pdf/nguyen2023tipi.pdf" target=&ldquo;blank&rdquo;>pdf</a>] 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=WOquZTLCBO1" target=&ldquo;blank&rdquo;><b></b><b>VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation</b><b></b></a><br /> 
<u>Thanh Nguyen-Tang</u>, Raman Arora <br /> 
International Conference on Learning Representations (<font color=red ><b></b><b>ICLR</b><b></b></font>), 2023 (<font color=red >Top 25% noble</font>)  
[<a href="https://arxiv.org/abs/2302.12780" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://recorder-v3.slideslive.com/?share=80688&amp;s=a24d555f-edda-4210-a19e-4fded4165c62" target=&ldquo;blank&rdquo;>talk</a>] [<a href="assets/viper.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="https://github.com/thanhnguyentang/neural-offline-rl" target=&ldquo;blank&rdquo;>code</a>]
</p>
</li>
</ul>
<ul>
<li><p><b></b><b>On Instance-Dependent Bounds for Offline Reinforcement Learning with Linear Function Approximation</b><b></b><br /> 
<u>Thanh Nguyen-Tang</u>, Ming Yin, Sunil Gupta, Svetha Venkatesh, Raman Arora <br /> 
AAAI Conference on Artificial Intelligence (<font color=red ><b></b><b>AAAI</b><b></b></font>), 2023 
[<a href="https://arxiv.org/abs/2211.13208" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="assets/aaai23-poster.pdf" target=&ldquo;blank&rdquo;>poster</a>] [<a href="assets/aaai23-slides-only.pdf" target=&ldquo;blank&rdquo;>slides</a>] [<a href="https://youtu.be/xUsMHodydO8" target=&ldquo;blank&rdquo;>video</a>]
</p>
</li>
</ul>
<h3>2022   </h3>
<ul>
<li><p><a href="https://openreview.net/forum?id=LdEm0umNcv" target=&ldquo;blank&rdquo;><b></b><b>On Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks in Besov Spaces</b><b></b></a><br />
<u>Thanh Nguyen-Tang</u>, Sunil Gupta, Hung Tran-The, Svetha Venkatesh <br /> 
Transactions on Machine Learning Research (<font color=red ><b></b><b>TMLR</b><b></b></font>), 2022 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=Yl_4LpR_3Z" target=&ldquo;blank&rdquo;><b></b><b>Improving Domain Generalization with Interpolation Robustness</b><b></b></a><br /> 
Ragja Palakkadavath, <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Svetha Venkatesh <br />
Distribution Shifts Workshop@NeurIPS2022, INTERPOLATE@NeurIPS2022 (<font color=red >Spotlight</font>)<br />
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=lTZBRxm2q5" target=&ldquo;blank&rdquo;><b></b><b>Learning Fractional White Noises in Neural Stochastic Differential Equations</b><b></b></a><br />  
Anh Tong, <u>Thanh Nguyen-Tang</u>, Toan Tran, Jaesik Choi <br />
Advances in Neural Information Processing Systems (<font color=red ><b></b><b>NeurIPS</b><b></b></font>), 2022 
[<a href="https://github.com/anh-tong/fractional_neural_sde" target=&ldquo;blank&rdquo;>code</a>] 
</p>
</li>
</ul>
<ul>
<li><p><b></b><b>Two-Stage Neural Contextual Bandits for Adaptive Personalised Recommendation</b><b></b><br />  
Mengyan Zhang, <u>Thanh Nguyen-Tang</u>, Fangzhao Wu, Zhenyu He, Xing Xie, Cheng Soon Ong <br />
Under review, 2022  
[<a href="https://arxiv.org/abs/2206.14648" target=&ldquo;blank&rdquo;>arXiv</a>]  
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=sPIFuucA3F" target=&ldquo;blank&rdquo;><b></b><b>Offline Neural Contextual Bandits:  Pessimism, Optimization and Generalization</b><b></b></a><br />
<u>Thanh Nguyen-Tang</u> , Sunil Gupta, A.Tuan Nguyen, and Svetha Venkatesh <br />  
International Conference on Learning Representations (<font color=red ><b></b><b>ICLR</b><b></b></font>), 2022 
[<a href="https://arxiv.org/abs/2111.13807" target=&ldquo;blank&rdquo;>arXiv</a>] 
[<a href="assets/poster_NeurIPSW21.pdf" target=&ldquo;blank&rdquo;>poster</a>]  
[<a href="assets/neuralcb_slides.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://github.com/thanhnguyentang/offline_neural_bandits" target=&ldquo;blank&rdquo;>code</a>] <br />
<a href="https://offline-rl-neurips.github.io/2021/pdf/28.pdf" target=&ldquo;blank&rdquo;>Workshop on OfflineRL</a>, NeurIPS, 2021 <br /> 
</p>
</li>
</ul>
<h3><b></b>2021<b></b> </h3>
<ul>
<li><p><a href="https://lyang36.github.io/icml2021_rltheory/camera_ready/5.pdf" target=&ldquo;blank&rdquo;><b></b><b>Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks</b><b></b></a> <br />
<u>Thanh Nguyen-Tang</u>, Sunil Gupta, Hung Tran-The, Svetha Venkatesh <br />
Workshop on Reinforcement Learning Theory, ICML, 2021
[<a href="https://arxiv.org/abs/2103.06671" target=&ldquo;blank&rdquo;>arXiv</a>]
[<a href="https://thanhnguyentang.github.io/assets/offrelu.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://www.youtube.com/watch?v=xLM5pondWY4" target=&ldquo;blank&rdquo;>talk</a>]
</p>
</li>
</ul>
<ul>
<li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17104" target=&ldquo;blank&rdquo;><b></b><b>Distributional Reinforcement Learning via Moment Matching</b><b></b></a><br /> 
<u>Thanh Nguyen-Tang</u>, Sunil Gupta, Svetha Venkatesh <br /> 
AAAI Conference on Artificial Intelligence (<font color=red ><b></b><b>AAAI</b><b></b></font>), 2021
[<a href="http://arxiv.org/abs/2007.12354" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/mmdrl" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://cutt.ly/fkkiAGm" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://cutt.ly/4kkiJZt" target=&ldquo;blank&rdquo;>poster</a>] [<a href="https://youtu.be/1fMqZZjy84E" target=&ldquo;blank&rdquo;>talk</a>] 
</p>
</li>
</ul>
<h3><b></b>2020<b></b></h3>
<ul>
<li><p><a href="http://proceedings.mlr.press/v108/nguyen20a.html" target=&ldquo;blank&rdquo;><b></b><b>Distributionally Robust Bayesian Quadrature Optimization</b><b></b></a><br /> 
<u>Thanh Nguyen-Tang</u>, Sunil Gupta, Huong Ha, Santu Rana, Svetha Venkatesh<br /> 
International Conference on Artificial Intelligence and Statistics (<font color=red ><b></b><b>AISTATS</b><b></b></font>), 2020 
[<a href="https://arxiv.org/abs/2001.06814" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/drbqo" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://thanhnguyentang.github.io/assets/aistats20_drbqo.pdf" target=&ldquo;blank&rdquo;>slides</a>] 
[<a href="https://slideslive.com/38930124/" target=&ldquo;blank&rdquo;>talk</a>]  
</p>
</li>
</ul>
<h3><b></b>2019<b></b> </h3>
<ul>
<li><p><a href="https://doi.org/10.3390/e21100976" target=&ldquo;blank&rdquo;><b></b><b>Markov Information Bottleneck to Improve Information Flow in Stochastic Neural Networks</b><b></b></a><br />
<u>Thanh Nguyen-Tang</u>, Jaesik Choi <br /><font color=red ><b></b><b>Entropy</b><b></b></font>, 21(10), 976, 2019 (in Special Issue on Information Bottleneck: Theory and Applications in Deep Learning) 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/9350-bayesian-optimization-with-unknown-search-space" target=&ldquo;blank&rdquo;><b></b><b>Bayesian Optimization with Unknown Search Space</b><b></b></a><br />
Huong Ha, Santu Rana, Sunil Gupta, <u>Thanh Nguyen-Tang</u>, Hung Tran-The, Svetha Venkatesh <br />
Advances in Neural Information Processing Systems (<font color=red ><b></b><b>NeurIPS</b><b></b></font>), 2019 
[<a href="https://github.com/HuongHa12/BO_unknown_searchspace" target=&ldquo;blank&rdquo;>code</a>]
[<a href="https://postersession.ai/poster/bayesian-optimization-with-unknown-searc/" target=&ldquo;blank&rdquo;>poster</a>]
</p>
</li>
</ul>
<h2>Academic Service  </h2>
<ul>
<li><p>Senior Program Committee: AAAI (2023)
</p>
</li>
<li><p>Reviewer/Program Committee: 
</p>
<ul>
<li><p>NeurIPS (<b></b><b>2023</b><b></b>, 2022, 2021, 2020)
</p>
</li>
<li><p>ICML (2023, 2022, 2021)
</p>
</li>
<li><p>ICLR (2023, 2022, 2021- outstanding reviewer award) 
</p>
</li>
<li><p>AAAI (2022, 2021-<a href="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2021/05/AAAI-21-Program-Committee.pdf" target=&ldquo;blank&rdquo;>Top 25% PC</a>, 2020)
</p>
</li>
<li><p>TPAMI (<b></b><b>2023</b><b></b>), AISTATS (2021), EWRL (2022), L4DC (2022), NeurIPS Workshop on OfflineRL (2022, 2021)
</p>
</li></ul>
</li>
<li><p>Volunteer: AAAI (2023 - session chair for ML theory), ICML (2022), AutoML (2022)   
</p>
</li>
</ul>
<p>* Bold means &ldquo;current&rdquo;
</p>
<h2>Teaching </h2>
<ul>
<li><p>Guest lecturer (in bandits/reinforcement learning): Machine Learning (CS 475/675) Spring 2023, JHU  
</p>
</li>
</ul>
<h2>Invited Talks </h2>
<ul>
<li><p>Offline Reinforcement Learning with Neural Function Approximation 
</p>
<ul>
<li><p>VinAI, Vietnam, Jan. 13, 2023 [<a href="https://www.vinai.io/seminar-posts/provable-offline-reinforcement-learning-neural-function-approximation-randomization-and-sample-complexity/" target=&ldquo;blank&rdquo;>post</a>]
</p>
</li>
<li><p>FPT AI, Vietnam, Dec. 21, 2022 [<a href="https://www.youtube.com/watch?v=FNj8UScJmrk" target=&ldquo;blank&rdquo;>record</a>]
</p>
</li>
<li><p>UC San Diego, USA, Dec. 8, 2022 (Host: <a href="https://roseyu.com/" target=&ldquo;blank&rdquo;>Rose Yu</a>)
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Offline Reinforcement Learning: Assurance in High-Stakes AI Applications [<a href="assets/offrl_iaa_22.pdf" target=&ldquo;blank&rdquo;>slides</a>]
</p>
<ul>
<li><p>IAA Research Summit, Johns Hopkins University, USA, Nov. 2022 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Offline Neural Contextual Bandits: Pessimism, Optimization, and Generalization
</p>
<ul>
<li><p>Ohio State University, USA, Jan. 2022 (Host: <a href="https://sites.google.com/view/yingbinliang/home" target=&ldquo;blank&rdquo;>Yingbin Liang</a> and <a href="http://newslab.ece.ohio-state.edu/home/" target=&ldquo;blank&rdquo;>Ness Shroff</a>)
</p>
</li>
<li><p>Arizona State University, USA, Dec. 2021 (Host: <a href="https://kwangsungjun.github.io/" target=&ldquo;blank&rdquo;>Kwang-Sung Jun</a>)
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>On Practical RL: Provable Robustness, Scalability, and Statistical Efficiency  
</p>
<ul>
<li><p>Virginia Tech, USA, Nov. 2021 (Host: <a href="https://sites.google.com/site/thinhdoan210/home" target=&ldquo;blank&rdquo;>Thinh T. Doan</a>)
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Generalization and Optimization in Deep Learning: Over-parameterization and Interpolation [<a href="assets/ntk_interpolation.pdf" target=&ldquo;blank&rdquo;>slides</a>]
</p>
<ul>
<li><p>Deakin University, Australia, Aug. 2021 
</p>
</li>
</ul>

</li>
</ul>
<h2>A guide to become a <u></u>great<u></u> researcher   </h2>
<ul>
<li><p>&ldquo;Judge a man by his questions rather than his answers.&rdquo; &ndash;Voltaire    
</p>
</li>
</ul>
<ul>
<li><p>&ldquo;To become a great researcher, don’t answer your questions, but question your answers. 
When you need to know the facts, you must inquire, not just make assumptions. 
Many people don’t want to ask questions because it exposes them to confront the reality of their circumstance, which may scare them. 
Moreover, asking questions forces them into the laborious task of thinking, which is why they fail to ask questions.&rdquo; &ndash;<a href="https://sites.harvard.edu/junliu/" target=&ldquo;blank&rdquo;>Jun Liu</a>
</p>
</li>
</ul>
<ul>
<li><p>Savage’s approach to research, via Mosteller (copy from Jun Liu’s webpage which is from Jon McAuliffe’s):
</p>
<ul>
<li><p>As soon as a problem is stated, start right away to solve it. Use simple examples.
</p>
</li>
<li><p>Keep starting from first principles, explaining again and again what you are trying to do.
</p>
</li>
<li><p>Believe that this problem can be solved and that you will enjoy working it out.
</p>
</li>
<li><p>Don’t be hampered by the original problem statement. Try other problems in its neighborhood; maybe there is a better problem than yours.
</p>
</li>
<li><p>Work an hour or so on it frequently. Talk about it; explain it to people.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>You and your research &ndash; <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html" target=&ldquo;blank&rdquo;>Richard Hamming</a>
</p>
<ul>
<li><p>Do not be afraid to do first-class research.
</p>
</li>
<li><p>Courage to think hard about your genuinely curious questions, dare to ask the impossible questions (Shannon’s case).
</p>
</li>
<li><p>Do not get the big thing right off, start with small basics and let it compound.
</p>
</li>
<li><p>Great scientists tolerate ambiguity.
</p>
</li>
<li><p>Take subconscious to your advantage (e.g. deeply immmerse into the problem, think of it in the sleep)
</p>
</li>
<li><p>Ask “What are the important problems in my field?”; keep a list of 10-20 important problems and look for an attack. 
</p>
</li>
<li><p>Learn to sell your work
</p>
</li>
<li><p>“Great thoughts time”: Devote some small time of one day in a week ONLY to think and understand the bigger problems in the field, what’s important, what’s not.
</p>
</li>
<li><p>Drive + commitment &gt; talent
</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-04-14 22:34:52 EDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</div>
</body>
</html>
