I"<p><a href="/">[Back Home]</a>  <a href="/blogs/post">[Back to Blog]</a></p>

<p>An unofficial collection of concentration inequalities that I find very helpful (and probably very common) despite of being simple. A motivation for this collection is to provide a cheatsheet for analyzing an algorithmâ€™s convergence rate and bounding some quantity of interest (specially in the context of sequential decision making under uncertainty):</p>
<h3 id="notation">Notation</h3>
<p>\(Pr(A) := Pr\{X \in A \} = P(\{ \omega \in \Omega: X(\omega) \in A \in \mathcal{B} \})\) where $X$ is a random variable that maps the probability space $(\Omega, \mathcal{F}, P)$ to the probability space $(\mathcal{X}, \mathcal{B}, Pr).$</p>
<h1 id="concetration-on-possitive-values">Concetration on possitive values</h1>

\[\mathbb{E}[X] \leq C Pr\{ X \geq 0\}\]

<p>where $C = \max_{X\in \mathcal{X}} X.$</p>

<h1 id="union-bound">Union bound</h1>

\[Pr( \cup_{i=1}^N A_i ) \leq \sum_{i=1}^{N} Pr( A_i)\]

<p>where $1 \leq N \leq \infty$. The equality occurs when $A_i$ are mutually non-overlapping.</p>

<h3 id="auxilary">Auxilary:</h3>

\[A \subset B \implies Pr(A) \leq Pr(B)\]

<h1 id="references">References</h1>
<p>[1] Russo and Van Roy. <em>Learning to Optimize via Posterior Sampling</em>.</p>

<p>(work in progress)</p>
:ET