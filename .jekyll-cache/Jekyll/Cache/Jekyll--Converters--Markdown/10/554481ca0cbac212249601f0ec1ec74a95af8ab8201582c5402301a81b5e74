I"–<p><a href="/">[Back Home]</a>  <a href="/blogs/post">[Back to Blog]</a></p>

<h1 id="preliminaries">Preliminaries</h1>

<ul>
  <li>The expected discounted reward:</li>
</ul>

\[J(\pi) = \mathbb{E}_{\mu(s)} \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \bigg| s_0 = s\right]\]

<ul>
  <li>The Bellman optimality operator:</li>
</ul>

\[T: \mathbb{R}^{\mathcal{S}} \rightarrow \mathbb{R}^{\mathcal{S}} \\ 
\forall s \in \mathcal{S}, V(s) \mapsto (TV)(s) := \max_{a \in \mathcal{A}} \{ R(s,a) + \gamma \mathbb{E}_{p(s'|s,a)} [V(s')] \}\]

<p>The optimal value function $V^* $ (i.e., $V^* \geq V, \forall V$) is the fixed point of $T$, i.e., $TV^* = V^*$. 
The optimal policy can then be constructed from the optimal value function: \(\pi^*(s) = arg\max_{a \in \mathcal{A}} \{ R(s,a) + \gamma \mathbb{E}_{p(s'|s,a)} [ V^{ * }(s')] \}\).</p>
<ul>
  <li>Given $V_1, V_2 \in \mathbb{R}^{\mathcal{S}}$, we denote ‚Äú$V_1(s) \geq V_2(s), \forall s \in \mathcal{S}$‚Äù by ‚Äú$V_1 \geq V_2$‚Äù.</li>
</ul>

<blockquote>
  <p><strong>Theorem</strong> (monotonicity): $T$ is monotonic, i.e., $V_1 \geq V_2 \implies TV_1 \geq TV_2$.</p>
</blockquote>

<blockquote>
  <p><strong>Theorem</strong> (contraction): \(\max_{s \in \mathcal{S}} |(TV_1)(s) - (TV_2)(s) | \leq \gamma \max_{s\in\mathcal{S}} |V_1(s) - V_2(s) |, \forall V_1, V_2.\)</p>
</blockquote>

<p><em>Proof</em>:</p>

\[|(TV_1)(s) - (TV_2)(s)| =  \bigg|\max_{a \in \mathcal{A}} \{ R(s,a) + \gamma \mathbb{E}_{p(s'|s,a)} [V_1(s')] \} -  \max_{a \in \mathcal{A}} \{ R(s,a) + \gamma \mathbb{E}_{p(s'|s,a)} [V_2(s')] \} \bigg| \\
\leq \gamma \max_{a \in \mathcal{A}} \bigg|  \mathbb{E}_{p(s'|s,a)} [V_1(s')] -  \mathbb{E}_{p(s'|s,a)} [V_2(s')]\bigg| \\ 
\leq \gamma  \max_{a \in \mathcal{A}} \bigg|  \mathbb{E}_{p(s'|s,a)} [  \max_{s\in\mathcal{S}} |V_1(s) - V_2(s) | ]\bigg| = \gamma \max_{s\in\mathcal{S}} |V_1(s) - V_2(s) |.\]

<blockquote>
  <p><strong>Auxilary</strong>: $T^{\infty}V = V^*, \forall V. $</p>
</blockquote>

<ul>
  <li>The Bellman optimality problem reframed as <strong>linear program</strong>:  The optimal value function $V^*$ is the unique solution of the following linear program,</li>
</ul>

\[P^* := \min_{V} (1 - \gamma) \mathbb{E}_{\mu(s)} [ V(s)] \\
\text{subject to } V(s) \geq R(s,a) + \gamma \mathbb{E}_{p(s'|s,a)} [V(s')], \forall (s,a).\]

<ul>
  <li>The <strong>dual form</strong> (which can be easily derived using [1]) of the LP above :</li>
</ul>

\[D^* := \max_{\lambda \geq 0} \sum_{s,a} \lambda(s,a) R(s,a) \\ 
\text{subject to } (1 - \gamma) \mu(s') + \gamma \sum_{s,a} \lambda(s,a) p(s' | s, a) = \sum_{a} \lambda(s',a), \forall s'.\]

<blockquote>
  <p><strong>Theorem</strong>: 
\(\sum_{s,a} \lambda^*(s,a) = 1, \pi^*(a|s) = \frac{\lambda^*(s,a)}{ \sum_{a'} \lambda^*(s,a')}.\)</p>
</blockquote>

<h1 id="references">References</h1>
<p>[1] https://thanhnguyentang.github.io/blogs/lagrangian<br />
[2] <a href="https://arxiv.org/abs/1712.10282">Boosting the Actor with Dual Critic</a></p>

<p>(work in progress)</p>
:ET