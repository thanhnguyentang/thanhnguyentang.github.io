<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Thanh Nguyen-Tang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<div id="layout-content">
</br>
 <h1>Thanh Nguyen-Tang</h1>
 <img alt="tnt" src="profile_pic.jpg"
style="display:inline;margin:5px 10px 0px 0px;float:center" width="180" id="profileimage">
<div id="subtitle">Postdoctoral Research Fellow <br />
<a href="https://www.cs.jhu.edu/" target=&ldquo;blank&rdquo;>Department of Computer Science</a> <br /> 
<a href="https://engineering.jhu.edu/" target=&ldquo;blank&rdquo;>Whiting School of Engineering</a> <br /> 
<a href="https://www.jhu.edu/" target=&ldquo;blank&rdquo;>Johns Hopkins University</a> <br /> <br />
3400 N Charles Street, Malone Hall 331, Baltimore, MD 21218 <br /> <br /> 
<u>Email</u> <i>nguyent</i> at cs dot jhu dot edu / <i>thnguyentang</i> at gmail dot com <br /> 
<u>Links</u> <a href="https://scholar.google.com/citations?hl=en&amp;user=UrTlMiwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=&ldquo;blank&rdquo;>Google Scholar</a>, 
<a href="https://www.semanticscholar.org/author/Thanh-Nguyen-Tang/1490490191?sort=pub-date" target=&ldquo;blank&rdquo;>Semantic Scholar</a>, 
<a href="https://github.com/thanhnguyentang" target=&ldquo;blank&rdquo;>Github</a>,
<a href="https://twitter.com/thanhnguyentang" target=&ldquo;blank&rdquo;>Twitter</a> 
</div>
<h2>About Me  </h2>
<p>I am a Postdoctoral Research Fellow in Department of Computer Science at Johns Hopkins University, 
working with <a href="https://www.cs.jhu.edu/~raman/Home.html" target=&ldquo;blank&rdquo;>Raman Arora</a>. I was an Associate Research Fellow at the Applied AI Institute, Deakin University in July 2021-June 2022 
and completed my PhD there in Feb 2022. I did my Master in Computer Science and Engineering at Ulsan National Institute of Science and Technology (UNIST) in 2018. 
</p>
<h2>Research Interests </h2>
<p>I am building toward data-efficient, runtime-efficient and robust AI 
by studying three foundational pillars of modern machine learning &ndash; provable statistical efficiency, computational efficiency, and robustness. My current focus includes: 
</p>
<ul>
<li><p>Reinforcement Learning 
</p>
</li>
<li><p>Learning under Distributional Shifts 
</p>
</li>
<li><p>Robust Adversarial Learning 
</p>
</li>
<li><p>Probabilistic Deep Learning 
</p>
</li>
<li><p>Representation Learning 
</p>
</li>
</ul>
<p>Iâ€™m always actively open to research collaborations and chat!
</p>
<h2>Recent News  </h2>
<ul>
<li><p>Nov. 19, 2022: One paper accepted to AAAI, 2023 (acceptance rate: 19.6%). 
</p>
</li>
<li><p>Oct. 30, 2022:  I was acknowledged in Francis Bach's <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target=&ldquo;blank&rdquo;>&ldquo;Learning Theory from First Principles&rdquo;</a>
</p>
</li>
<li><p>Sep. 14, 2022: One paper got accepted to NeurIPS, 2022. 
</p>
</li>
<li><p>Aug. 8, 2022: I am acknowledged in Mengyan Zhang's PhD thesis. 
</p>
</li>
<li><p>Jan. 21, 2022: One paper got accepted to ICLR, 2022. 
</p>
</li>
<li><p>Oct. 25, 2021: A short version of our work has been accepted to the NeurIPS&rsquo;21 Workshop on Offline Reinforcement Learning.
</p>
</li>
<li><p>July 8, 2021: A short version of our work has been accepted to the ICML&rsquo;21 Workshop on Reinforcement Learning Theory.
</p>
</li>
<li><p>July 1, 2021: I start my postdoc at A\(^2\)I\(^2\), Deakin University after submitting my Ph.D. thesis in 24 Jun. 
</p>
</li>
<li><p>May 20, 2021: I have been accepted to the Deep Learning Theory Summer School at Princeton, acceptance rate: 180/500 = 36%.
</p>
</li>
</ul>
<h2>Publications  </h2>
<h3>2023  </h3>
<ul>
<li><p>On Instance-Dependent Bounds for Offline Reinforcement Learning with Linear Function Approximation<br /> 
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Ming Yin, Sunil Gupta, Svetha Venkatesh, Raman Arora <br /> 
</p>
</li>
<li><p>AAAI, 2023 <br />
</p>
</li>
</ul>

</li>
</ul>
<h3>2022  </h3>
<ul>
<li><p>TIPI: Test Time Adaptation with Transformation Invariance<br /> 
</p>
<ul>
<li><p>A. Tuan Nguyen, <u>Thanh Nguyen-Tang</u>, Ser-Nam Lim, Philip Torr <br />
</p>
</li>
<li><p>Under review, 2022 <br />
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=Yl_4LpR_3Z" target=&ldquo;blank&rdquo;>Improving Domain Generalization with Interpolation Robustness</a><br /> 
</p>
<ul>
<li><p>Ragja Palakkadavath, <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Svetha Venkatesh <br />
</p>
</li>
<li><p>Distribution Shifts Workshop@NeurIPS2022, INTERPOLATE@NeurIPS2022 (<i>Spotlight</i>)<br />
</p>
</li>
<li><p>Under review, 2022 <br />
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Provably Efficient Neural Offline Reinforcement Learning via Perturbed Rewards<br /> 
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Raman Arora <br /> 
</p>
</li>
<li><p>Under review, 2022 <br />
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=lTZBRxm2q5" target=&ldquo;blank&rdquo;>Learning Fractional White Noises in Neural Stochastic Differential Equations</a><br />  
</p>
<ul>
<li><p>Anh Tong, <u>Thanh Nguyen-Tang</u>, Toan Tran, Jaesik Choi <br />
</p>
</li>
<li><p>NeurIPS, 2022 <br /> 
</p>
</li>
<li><p>[<a href="https://github.com/anh-tong/fractional_neural_sde" target=&ldquo;blank&rdquo;>Code</a>] 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Two-Stage Neural Contextual Bandits for Adaptive Personalised Recommendation<br />  
</p>
<ul>
<li><p>Mengyan Zhang, <u>Thanh Nguyen-Tang</u>, Fangzhao Wu, Zhenyu He, Xing Xie, Cheng Soon Ong <br />
</p>
</li>
<li><p>Under review, 2022 <br /> 
</p>
</li>
<li><p>[<a href="https://arxiv.org/abs/2206.14648" target=&ldquo;blank&rdquo;>arXiv</a>]  
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Contextual Bandits with Reduced Explorations via Logged Data<br />
</p>
<ul>
<li><p>Hung Tran-The, <u>Thanh Nguyen-Tang</u>, Sunil Gupta, Santu Rana, Svetha Venkatesh <br /> 
</p>
</li>
<li><p>Under review, 2022 <br />
</p>
</li>
<li><p>[<a href="https://arxiv.org/abs/2107.11533" target=&ldquo;blank&rdquo;>arXiv</a>]
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=sPIFuucA3F" target=&ldquo;blank&rdquo;>Offline Neural Contextual Bandits:  Pessimism, Optimization and Generalization</a><br />
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u> , Sunil Gupta, A.Tuan Nguyen, and Svetha Venkatesh <br />  
</p>
</li>
<li><p>ICLR, 2022 <br /> 
</p>
</li>
<li><p><a href="https://offline-rl-neurips.github.io/2021/pdf/28.pdf" target=&ldquo;blank&rdquo;>Workshop on OfflineRL</a>, NeurIPS, 2021 <br /> 
</p>
</li>
<li><p>[<a href="https://arxiv.org/abs/2111.13807" target=&ldquo;blank&rdquo;>arXiv</a>] 
[<a href="assets/poster_NeurIPSW21.pdf" target=&ldquo;blank&rdquo;>Poster</a>]  
[<a href="assets/neuralcb_slides.pdf" target=&ldquo;blank&rdquo;>Slides</a>] 
[<a href="https://github.com/thanhnguyentang/offline_neural_bandits" target=&ldquo;blank&rdquo;>Code</a>]
</p>
</li>
</ul>

</li>
</ul>
<h3><b></b>2021<b></b> </h3>
<ul>
<li><p><a href="https://lyang36.github.io/icml2021_rltheory/camera_ready/5.pdf" target=&ldquo;blank&rdquo;>Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks</a> <br />
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Sunil Gupta, Hung Tran-The, Svetha Venkatesh <br />
</p>
</li>
<li><p>Workshop on Reinforcement Learning Theory, ICML, 2021 <br />
</p>
</li>
<li><p>[<a href="https://arxiv.org/abs/2103.06671" target=&ldquo;blank&rdquo;>arXiv</a>]
[<a href="https://thanhnguyentang.github.io/assets/offrelu.pdf" target=&ldquo;blank&rdquo;>Slides</a>] 
[<a href="https://www.youtube.com/watch?v=xLM5pondWY4" target=&ldquo;blank&rdquo;>Talk</a>]
</p>
</li></ul>
</li>
<li><p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17104" target=&ldquo;blank&rdquo;>Distributional Reinforcement Learning via Moment Matching</a><br /> 
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Sunil Gupta, Svetha Venkatesh <br /> 
</p>
</li>
<li><p>AAAI, 2021 <br />
</p>
</li>
<li><p>[<a href="http://arxiv.org/abs/2007.12354" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/mmdrl" target=&ldquo;blank&rdquo;>Code</a>]
[<a href="https://cutt.ly/fkkiAGm" target=&ldquo;blank&rdquo;>Slides</a>] 
[<a href="https://cutt.ly/4kkiJZt" target=&ldquo;blank&rdquo;>Poster</a>] [<a href="https://youtu.be/1fMqZZjy84E" target=&ldquo;blank&rdquo;>Talk</a>] 
</p>
</li>
</ul>

</li>
</ul>
<h3><b></b>2020<b></b></h3>
<ul>
<li><p><a href="http://proceedings.mlr.press/v108/nguyen20a.html" target=&ldquo;blank&rdquo;>Distributionally Robust Bayesian Quadrature Optimization</a><br /> 
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Sunil Gupta, Huong Ha, Santu Rana, Svetha Venkatesh<br /> 
</p>
</li>
<li><p>AISTATS, 2020 <br /> 
</p>
</li>
<li><p>[<a href="https://arxiv.org/abs/2001.06814" target=&ldquo;blank&rdquo;>arXiv</a>] [<a href="https://github.com/thanhnguyentang/drbqo" target=&ldquo;blank&rdquo;>Code</a>]
[<a href="https://thanhnguyentang.github.io/assets/aistats20_drbqo.pdf" target=&ldquo;blank&rdquo;>Slides</a>] 
[<a href="https://slideslive.com/38930124/" target=&ldquo;blank&rdquo;>Talk</a>]  
</p>
</li>
</ul>

</li>
</ul>
<h3><b></b>2019<b></b> </h3>
<ul>
<li><p><a href="https://doi.org/10.3390/e21100976" target=&ldquo;blank&rdquo;>Markov Information Bottleneck to Improve Information Flow in Stochastic Neural Networks</a><br />
</p>
<ul>
<li><p><u>Thanh Nguyen-Tang</u>, Jaesik Choi<br />
</p>
</li>
<li><p>Entropy, 21(10), 976, 2019 <br /> 
</p>
</li>
<li><p>[<a href="https://github.com/thanhnguyentang/pib" target=&ldquo;blank&rdquo;>Code</a>]
</p>
</li></ul>
</li>
<li><p><a href="https://papers.nips.cc/paper/9350-bayesian-optimization-with-unknown-search-space" target=&ldquo;blank&rdquo;>Bayesian Optimization with Unknown Search Space</a><br />
</p>
<ul>
<li><p>Huong Ha, Santu Rana, Sunil Gupta, <u>Thanh Nguyen-Tang</u>, Hung Tran-The, Svetha Venkatesh <br />
</p>
</li>
<li><p>NeurIPS, 2019 <br />
</p>
</li>
<li><p>[<a href="https://github.com/HuongHa12/BO_unknown_searchspace" target=&ldquo;blank&rdquo;>Code</a>]
[<a href="https://postersession.ai/poster/bayesian-optimization-with-unknown-searc/" target=&ldquo;blank&rdquo;>Poster</a>]
</p>
</li>
</ul>

</li>
</ul>
<h3>Dissertations </h3>
<ul>
<li><p><a href="https://thanhnguyentang.github.io/" target=&ldquo;blank&rdquo;>On Practical Reinforcement Learning: Provable Robustness, Scalability and Statistical Efficiency</a><br /> 
</p>
<ul>
<li><p>Ph.D. dissertation, Deakin University, Australia, July 2021
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="http://scholarworks.unist.ac.kr/handle/201301/23561" target=&ldquo;blank&rdquo;>Parametric Information Bottleneck to Optimize Stochastic Neural Networks</a><br />
</p>
<ul>
<li><p>Master Thesis, Ulsan National University of Science and Technology, South Korea, 2018<br />
</p>
</li>
<li><p>[<a href="https://thanhnguyentang.github.io/assets/PIB_thesis_slide.pdf" target=&ldquo;blank&rdquo;>Slides</a>] [<a href="https://www.youtube.com/watch?v=md9qV4Hrgbo&amp;t=378s" target=&ldquo;blank&rdquo;>Talk</a>]
</p>
</li>
</ul>

</li>
</ul>
<h2>Academic Service  </h2>
<ul>
<li><p>Senior Program Committee: AAAI (2023)
</p>
</li>
<li><p>Reviewer/Program Committee: NeurIPS (2022, 2021, 2020), ICML (2022, 2021), ICLR (2023, 2022, 2021- Outstanding reviewer award), AISTATS (2021), 
AAAI (2022, 2021-<a href="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2021/05/AAAI-21-Program-Committee.pdf" target=&ldquo;blank&rdquo;>Top 25% of Program Committees</a>, 2020), 
EWRL (2022), L4DC (2022), NeurIPS Workshop on OfflineRL (2022, 2021)
</p>
</li>
<li><p>Volunteer: ICML (2022), AutoML (2022) 
</p>
</li>
</ul>
<h2>Invited Talks </h2>
<ul>
<li><p>Provable Offline Reinforcement Learning: Neural Function Approximation, Randomization, and Sample Complexity 
</p>
<ul>
<li><p>VinAI, Vietnam, Jan. 13, 2023 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Provable Offline Reinforcement Learning: Neural Function Approximation, Randomization, and Sample Complexity 
</p>
<ul>
<li><p>FPT AI, Vietnam, Dec. 21, 2022 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Provable Offline Reinforcement Learning: Neural Function Approximation, Randomization, and Sample Complexity 
</p>
<ul>
<li><p>UC San Diego, USA, Dec. 8, 2022 (Host: Rose Yu)
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Offline Reinforcement Learning: Assurance in High-Stakes AI Applications 
</p>
<ul>
<li><p>IAA Research Summit, Johns Hopkins University, USA, Nov. 2022 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Offline Neural Contextual Bandits: Pessimism, Optimization, and Generalization
</p>
<ul>
<li><p>Ohio State University, USA, Jan. 2022 (Host: Yingbin Liang and Ness Shroff)
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Offline Neural Contextual Bandits: Pessimism, Optimization, and Generalization
</p>
<ul>
<li><p>Arizona State University, USA, Dec. 2021 (Host: Kwang-Sung Jun)
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Generalization and Optimization in Deep Learning: Over-parameterization and Interpolation
</p>
<ul>
<li><p>Deakin University, Australia, Aug. 2021 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>On Finite-Sample Analysis of Batch Reinforcement Learning with Deep ReLU Networks
</p>
<ul>
<li><p>Viet Operator Theorists Group, Vietnam and USA, Apr. 2021
</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-11-19 09:59:03 EST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</div>
</body>
</html>
